# src/agents/malware_agent.py
"""
Malware Payload Agent
Purpose: Detects malware, malicious payloads, exploit kits, and suspicious files
Techniques: YARA rules, signature matching, heuristic analysis, sandboxing, ML models
"""

import re
import hashlib
import base64
from typing import Dict, List, Any, Set, Optional
import numpy as np
from datetime import datetime
import time
import json
import gzip
import io

from .base_agent import SecurityAgent, AgentCapability

class MalwarePayloadAgent(SecurityAgent):
    """
    Malware Payload Detection Agent
    
    This agent specializes in detecting:
    1. Web shells and backdoors
    2. Exploit kits and payloads
    3. Malicious file uploads
    4. Obfuscated JavaScript
    5. Command injection attempts
    6. PHP malware
    7. SQL injection payloads
    8. Cross-site scripting (XSS) payloads
    9. Remote code execution attempts
    10. File inclusion attacks
    
    Techniques used:
    - YARA rule matching
    - Signature-based detection
    - Heuristic analysis
    - Behavioral analysis
    - Sandbox execution (optional)
    - Machine learning models
    - Entropy analysis
    - Pattern matching
    """
    
    def __init__(self, agent_id: str = "malware_agent_001"):
        super().__init__(
            agent_id=agent_id,
            name="Malware Payload Agent",
            state_dim=512
        )
        
        # Add malware detection capability
        self.capabilities.append(AgentCapability.MALWARE_DETECTION)
        
        # YARA rules database
        self.yara_rules = self._load_yara_rules()
        
        # Malware signatures database
        self.malware_signatures = self._load_malware_signatures()
        
        # File type detection
        self.file_signatures = {
            'php': b'<?php',
            'javascript': b'<script',
            'executable': b'MZ',  # Windows EXE
            'elf': b'\x7fELF',    # Linux ELF
            'pdf': b'%PDF-',
            'zip': b'PK\x03\x04',
            'gzip': b'\x1f\x8b',
            'rar': b'Rar!\x1a\x07'
        }
        
        # Detection thresholds
        self.thresholds = {
            'entropy_threshold': 7.5,  # High entropy = possible encryption/compression
            'shell_pattern_confidence': 0.8,
            'exploit_confidence': 0.9,
            'obfuscation_confidence': 0.7,
            'min_signature_matches': 2,
            'max_file_size': 10 * 1024 * 1024,  # 10MB
        }
        
        # Heuristic detection patterns
        self.heuristic_patterns = {
            'web_shell': [
                r'eval\s*\(.*base64_decode',
                r'system\s*\(.*\$_(GET|POST)',
                r'shell_exec\s*\(.*\$_(GET|POST)',
                r'passthru\s*\(.*\$_(GET|POST)',
                r'exec\s*\(.*\$_(GET|POST)',
                r'assert\s*\(.*\$_(GET|POST)',
                r'popen\s*\(.*\$_(GET|POST)',
                r'proc_open\s*\(.*\$_(GET|POST)'
            ],
            'command_injection': [
                r';\s*(ls|dir|cat|type|rm|del)',
                r'\|\s*(ls|dir|cat|type|rm|del)',
                r'`\s*(ls|dir|cat|type|rm|del)',
                r'\$\s*\(.*(ls|dir|cat|type|rm|del)',
                r'(nc|netcat|wget|curl|python|perl|bash).*\$',
                r'(127\.0\.0\.1|localhost).*\|\s*sh'
            ],
            'php_malware': [
                r'\$[a-z_]\s*=\s*["\'].*["\']\s*;\s*eval\s*\(\$[a-z_]',
                r'gzinflate\s*\(.*base64_decode',
                r'str_rot13\s*\(.*eval',
                r'create_function\s*\(.*\$_(GET|POST)',
                r'preg_replace\s*\(.*/e.*\$_(GET|POST)'
            ],
            'obfuscated_js': [
                r'eval\s*\(.*String\.fromCharCode',
                r'unescape\s*\(.*%',
                r'document\.write\s*\(.*atob',
                r'window\[.*\]\s*\(.*eval',
                r'Function\s*\(.*return'
            ],
            'sql_injection': [
                r"'\s+OR\s+['1'=\s*'1']",
                r"UNION\s+SELECT",
                r";\s*(DROP|DELETE|TRUNCATE)\s+",
                r"OR\s+['1'=\s*'1']--",
                r"WAITFOR\s+DELAY\s+['\"][0-9]"
            ],
            'xss_payload': [
                r'<script>.*alert\s*\(.*</script>',
                r'onload\s*=\s*["\'].*alert',
                r'onerror\s*=\s*["\'].*alert',
                r'javascript:\s*alert',
                r'<iframe.*src\s*=\s*["\']javascript:'
            ]
        }
        
        # File upload analysis
        self.suspicious_extensions = {
            'high_risk': ['.php', '.phtml', '.phar', '.inc', '.pl', '.cgi', '.exe', '.bat', '.cmd', '.sh'],
            'medium_risk': ['.jsp', '.asp', '.aspx', '.jar', '.war', '.py', '.rb', '.ps1'],
            'low_risk': ['.html', '.htm', '.js', '.css', '.txt', '.pdf', '.doc', '.xls']
        }
        
        # Metrics
        self.metrics = {
            'files_analyzed': 0,
            'malware_detected': 0,
            'false_positives': 0,
            'yara_matches': 0,
            'heuristic_matches': 0,
            'avg_analysis_time': 0.0
        }
        
        # Known malware hashes (in production, load from database)
        self.malware_hashes = set()
        
        # ML model for malware detection
        self._load_ml_model()
    
    def _load_yara_rules(self) -> List[Dict]:
        """
        Load YARA rules for malware detection
        
        YARA is a pattern matching tool used by malware researchers
        """
        # In production, load from YARA rule files
        # For now, create some example rules
        return [
            {
                'name': 'WebShell_PHP',
                'pattern': r'eval\s*\(\s*base64_decode\s*\(\s*["\'][A-Za-z0-9+/=]+["\']\s*\)\s*\)',
                'description': 'PHP webshell using base64 encoded eval',
                'severity': 'HIGH'
            },
            {
                'name': 'JavaScript_Obfuscation',
                'pattern': r'eval\s*\(\s*unescape\s*\(\s*["\']%[0-9A-F]{2,}["\']\s*\)\s*\)',
                'description': 'Obfuscated JavaScript using unescape',
                'severity': 'MEDIUM'
            },
            {
                'name': 'Command_Injection',
                'pattern': r';\s*(wget|curl|nc|netcat|python|perl|bash)\s+',
                'description': 'Command injection attempt',
                'severity': 'CRITICAL'
            }
        ]
    
    def _load_malware_signatures(self) -> Dict[str, List[str]]:
        """
        Load malware signatures database
        
        Signatures include:
        - Known malware strings
        - Exploit kit patterns
        - Backdoor signatures
        - C2 communication patterns
        """
        return {
            'web_shells': [
                'c99shell', 'r57shell', 'wso', 'b374k',
                'c100', 'phpspy', 'acid', 'krypton'
            ],
            'exploit_kits': [
                'angler', 'nuclear', 'magnitude', 'rig',
                'sundown', 'kryptonite', 'terror', 'redir'
            ],
            'backdoors': [
                'backdoor.php', 'shell.php', 'cmd.php',
                'admin.php', 'config.php', 'wp-config.php'
            ],
            'c2_patterns': [
                'http://', 'https://', 'tcp://', 'udp://',
                '.onion', '.xyz', '.top', '.club'
            ]
        }
    
    def _load_ml_model(self):
        """
        Load machine learning model for malware detection
        
        Features include:
        - File entropy
        - String patterns
        - Byte distribution
        - Structural features
        """
        # In production, load pre-trained model
        # self.model = joblib.load('models/malware_detection_model.pkl')
        self.model = None
        print(f"✅ {self.name}: Malware detection ML model placeholder initialized")
    
    def analyze(self, security_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze payload for malware
        
        Multi-stage analysis:
        1. File type detection
        2. Hash checking
        3. YARA rule matching
        4. Signature matching
        5. Heuristic analysis
        6. Entropy analysis
        7. ML model prediction
        8. Sandbox analysis (if enabled)
        """
        start_time = time.time()
        
        try:
            # Extract payload data
            payload = security_data.get('payload', {})
            
            if not payload:
                return {
                    'agent_id': self.agent_id,
                    'agent_name': self.name,
                    'is_malware': False,
                    'confidence': 0.1,
                    'reason': 'No payload to analyze'
                }
            
            # Update metrics
            self.metrics['files_analyzed'] += 1
            
            # Get payload content
            content = self._get_payload_content(payload)
            filename = payload.get('filename', 'unknown')
            file_size = len(content) if content else 0
            
            # Initialize detection results
            detections = []
            malware_confidence = 0.0
            malware_type = "unknown"
            
            # Stage 1: File size check
            if file_size > self.thresholds['max_file_size']:
                detections.append({
                    'stage': 'file_size',
                    'type': 'OVERSIZED_FILE',
                    'confidence': 0.3,
                    'evidence': f'File size {file_size} exceeds limit'
                })
                malware_confidence = max(malware_confidence, 0.3)
            
            # Stage 2: File type detection
            file_type = self._detect_file_type(content, filename)
            risk_level = self._get_file_risk_level(filename, file_type)
            
            if risk_level == 'high_risk':
                detections.append({
                    'stage': 'file_type',
                    'type': 'HIGH_RISK_FILE_TYPE',
                    'confidence': 0.6,
                    'evidence': f'High risk file type: {file_type}'
                })
                malware_confidence = max(malware_confidence, 0.6)
            
            # Stage 3: Hash checking
            if content:
                file_hash = hashlib.sha256(content).hexdigest()
                if file_hash in self.malware_hashes:
                    detections.append({
                        'stage': 'hash',
                        'type': 'KNOWN_MALWARE_HASH',
                        'confidence': 0.95,
                        'evidence': f'Known malware hash: {file_hash[:16]}...'
                    })
                    malware_confidence = max(malware_confidence, 0.95)
                    malware_type = 'known_malware'
            
            # Stage 4: YARA rule matching
            if content:
                yara_detections = self._apply_yara_rules(content)
                detections.extend(yara_detections)
                if yara_detections:
                    self.metrics['yara_matches'] += len(yara_detections)
                    max_yara_confidence = max((d.get('confidence', 0) for d in yara_detections), default=0)
                    malware_confidence = max(malware_confidence, max_yara_confidence)
            
            # Stage 5: Signature matching
            if content:
                signature_detections = self._match_signatures(content)
                detections.extend(signature_detections)
                if signature_detections:
                    max_sig_confidence = max((d.get('confidence', 0) for d in signature_detections), default=0)
                    malware_confidence = max(malware_confidence, max_sig_confidence)
            
            # Stage 6: Heuristic analysis
            if content:
                heuristic_detections = self._heuristic_analysis(content, filename)
                detections.extend(heuristic_detections)
                if heuristic_detections:
                    self.metrics['heuristic_matches'] += len(heuristic_detections)
                    max_heuristic_confidence = max((d.get('confidence', 0) for d in heuristic_detections), default=0)
                    malware_confidence = max(malware_confidence, max_heuristic_confidence)
            
            # Stage 7: Entropy analysis
            if content:
                entropy_detections = self._entropy_analysis(content)
                detections.extend(entropy_detections)
                if entropy_detections:
                    max_entropy_confidence = max((d.get('confidence', 0) for d in entropy_detections), default=0)
                    malware_confidence = max(malware_confidence, max_entropy_confidence)
            
            # Stage 8: ML model prediction (if available)
            if self.model and content:
                ml_detection = self._ml_predict(content, filename)
                if ml_detection:
                    detections.append(ml_detection)
                    malware_confidence = max(malware_confidence, ml_detection.get('confidence', 0))
            
            # Determine if malware
            is_malware = malware_confidence > 0.7
            
            if is_malware:
                self.metrics['malware_detected'] += 1
            
            # Update agent confidence
            certainty = 0.5 + (malware_confidence * 0.5) if is_malware else 0.5
            self.update_confidence({'certainty': certainty})
            
            # Calculate processing time
            processing_time = time.time() - start_time
            self._update_avg_time(processing_time)
            
            # Generate response
            response = {
                'agent_id': self.agent_id,
                'agent_name': self.name,
                'analysis_timestamp': datetime.now().isoformat(),
                'processing_time': processing_time,
                'file_info': {
                    'filename': filename,
                    'size': file_size,
                    'type': file_type,
                    'risk_level': risk_level,
                    'hash': file_hash if content else None
                },
                'is_malware': is_malware,
                'malware_confidence': malware_confidence,
                'malware_type': malware_type,
                'detections': detections,
                'total_detections': len(detections),
                'recommended_action': self._get_recommended_action(is_malware, malware_confidence, risk_level),
                'sandbox_required': self._requires_sandbox_analysis(detections, file_type),
                'reasoning_state': self.get_reasoning_state(),
                'decision': {
                    'is_malware': is_malware,
                    'confidence': malware_confidence,
                    'malware_type': malware_type,
                    'evidence': detections[:5]  # Top 5 detections
                }
            }
            
            return response
            
        except Exception as e:
            print(f"❌ {self.name}: Malware analysis error: {e}")
            return self._error_response(str(e))
    
    def _get_payload_content(self, payload: Dict) -> Optional[bytes]:
        """
        Extract content from payload data
        
        Handles different encodings:
        - Raw bytes
        - Base64 encoded
        - Gzip compressed
        - Multipart form data
        """
        content = payload.get('content')
        
        if content is None:
            return None
        
        # If content is string, try to decode
        if isinstance(content, str):
            # Try base64 decode
            try:
                if len(content) % 4 == 0 and re.match(r'^[A-Za-z0-9+/]*={0,2}$', content):
                    return base64.b64decode(content)
            except:
                pass
            
            # Return as bytes
            return content.encode('utf-8', errors='ignore')
        
        # If already bytes
        if isinstance(content, bytes):
            # Try to decompress if gzipped
            if content[:2] == b'\x1f\x8b':
                try:
                    return gzip.decompress(content)
                except:
                    pass
            
            return content
        
        return None
    
    def _detect_file_type(self, content: Optional[bytes], filename: str) -> str:
        """
        Detect file type from content and filename
        """
        # Check file extension first
        ext = self._get_file_extension(filename).lower()
        
        if ext:
            return ext
        
        # Check content signatures
        if content:
            for file_type, signature in self.file_signatures.items():
                if content.startswith(signature):
                    return file_type
        
        return 'unknown'
    
    def _get_file_extension(self, filename: str) -> str:
        """
        Extract file extension from filename
        """
        if '.' in filename:
            return filename.split('.')[-1]
        return ''
    
    def _get_file_risk_level(self, filename: str, file_type: str) -> str:
        """
        Determine risk level based on file type and extension
        """
        ext = self._get_file_extension(filename).lower()
        
        # Check high risk extensions
        if ext in self.suspicious_extensions['high_risk']:
            return 'high_risk'
        
        # Check medium risk extensions
        if ext in self.suspicious_extensions['medium_risk']:
            return 'medium_risk'
        
        # Check file type
        if file_type in ['php', 'exe', 'elf']:
            return 'high_risk'
        
        if file_type in ['javascript', 'jar', 'war']:
            return 'medium_risk'
        
        return 'low_risk'
    
    def _apply_yara_rules(self, content: bytes) -> List[Dict]:
        """
        Apply YARA rules to content
        
        YARA rules use regular expressions for pattern matching
        """
        detections = []
        content_str = content.decode('utf-8', errors='ignore')
        
        for rule in self.yara_rules:
            pattern = rule['pattern']
            
            try:
                if re.search(pattern, content_str, re.IGNORECASE | re.DOTALL):
                    confidence = self._get_severity_confidence(rule.get('severity', 'MEDIUM'))
                    
                    detections.append({
                        'stage': 'yara',
                        'type': rule['name'],
                        'confidence': confidence,
                        'evidence': rule['description'],
                        'severity': rule.get('severity', 'MEDIUM')
                    })
            except re.error:
                # Invalid regex pattern
                continue
        
        return detections
    
    def _get_severity_confidence(self, severity: str) -> float:
        """
        Convert severity string to confidence score
        """
        severity_map = {
            'CRITICAL': 0.95,
            'HIGH': 0.8,
            'MEDIUM': 0.6,
            'LOW': 0.4,
            'INFO': 0.2
        }
        return severity_map.get(severity.upper(), 0.5)
    
    def _match_signatures(self, content: bytes) -> List[Dict]:
        """
        Match against known malware signatures
        """
        detections = []
        content_str = content.decode('utf-8', errors='ignore').lower()
        
        for category, signatures in self.malware_signatures.items():
            for signature in signatures:
                if signature.lower() in content_str:
                    confidence = 0.8 if category in ['web_shells', 'exploit_kits'] else 0.6
                    
                    detections.append({
                        'stage': 'signature',
                        'type': f'{category.upper()}_SIGNATURE',
                        'confidence': confidence,
                        'evidence': f'Found {category} signature: {signature}',
                        'signature': signature
                    })
        
        return detections
    
    def _heuristic_analysis(self, content: bytes, filename: str) -> List[Dict]:
        """
        Perform heuristic analysis for malware patterns
        """
        detections = []
        content_str = content.decode('utf-8', errors='ignore')
        
        for pattern_type, patterns in self.heuristic_patterns.items():
            matches = []
            
            for pattern in patterns:
                try:
                    if re.search(pattern, content_str, re.IGNORECASE):
                        matches.append(pattern)
                except re.error:
                    continue
            
            if matches:
                confidence = self._get_heuristic_confidence(pattern_type, len(matches))
                
                detections.append({
                    'stage': 'heuristic',
                    'type': f'{pattern_type.upper()}_PATTERN',
                    'confidence': confidence,
                    'evidence': f'Found {len(matches)} {pattern_type} patterns',
                    'patterns_found': matches[:3]  # First 3 patterns
                })
        
        return detections
    
    def _get_heuristic_confidence(self, pattern_type: str, match_count: int) -> float:
        """
        Get confidence score for heuristic detection
        """
        base_confidence = {
            'web_shell': 0.8,
            'command_injection': 0.9,
            'php_malware': 0.7,
            'obfuscated_js': 0.6,
            'sql_injection': 0.5,
            'xss_payload': 0.4
        }.get(pattern_type, 0.5)
        
        # More matches = higher confidence
        match_factor = min(1.0, match_count / 5.0)
        
        return min(1.0, base_confidence + (match_factor * 0.2))
    
    def _entropy_analysis(self, content: bytes) -> List[Dict]:
        """
        Analyze file entropy for encryption/compression detection
        
        High entropy can indicate:
        - Encrypted payloads
        - Compressed malware
        - Packed executables
        """
        if len(content) < 100:
            return []
        
        # Calculate Shannon entropy
        byte_counts = [0] * 256
        for byte in content:
            byte_counts[byte] += 1
        
        entropy = 0.0
        total_bytes = len(content)
        
        for count in byte_counts:
            if count > 0:
                probability = count / total_bytes
                entropy -= probability * np.log2(probability)
        
        # Maximum entropy for bytes is 8.0
        entropy_normalized = entropy / 8.0
        
        detections = []
        
        if entropy_normalized > 0.9:  # Very high entropy
            detections.append({
                'stage': 'entropy',
                'type': 'HIGH_ENTROPY',
                'confidence': 0.8,
                'evidence': f'Very high entropy ({entropy:.2f}/8.0) - possible encryption',
                'entropy': entropy,
                'normalized': entropy_normalized
            })
        elif entropy_normalized > self.thresholds['entropy_threshold'] / 8.0:
            detections.append({
                'stage': 'entropy',
                'type': 'ELEVATED_ENTROPY',
                'confidence': 0.5,
                'evidence': f'Elevated entropy ({entropy:.2f}/8.0)',
                'entropy': entropy,
                'normalized': entropy_normalized
            })
        
        return detections
    
    def _ml_predict(self, content: bytes, filename: str) -> Optional[Dict]:
        """
        Use ML model to predict if file is malware
        
        This is a placeholder for actual ML implementation
        """
        if not self.model:
            return None
        
        # In production, extract features and run through model
        features = self._extract_ml_features(content, filename)
        
        # Placeholder: Random prediction for demo
        is_malware = np.random.random() > 0.8  # 20% chance of malware
        confidence = np.random.random()
        
        if is_malware:
            return {
                'stage': 'ml_model',
                'type': 'ML_MALWARE_PREDICTION',
                'confidence': confidence,
                'evidence': f'ML model predicts malware (confidence: {confidence:.2f})'
            }
        
        return None
    
    def _extract_ml_features(self, content: bytes, filename: str) -> List[float]:
        """
        Extract features for ML model
        
        Features include:
        - File size statistics
        - Byte distribution
        - String features
        - Entropy measures
        - Structural features
        """
        features = []
        
        # File size features
        file_size = len(content)
        features.append(min(1.0, file_size / 10000000))  # Normalized size
        
        # Byte distribution features
        if file_size > 0:
            byte_counts = [0] * 256
            for byte in content:
                byte_counts[byte] += 1
            
            # Add normalized byte frequencies
            for i in range(0, 256, 16):  # Sample every 16th byte frequency
                freq_sum = sum(byte_counts[i:i+16])
                features.append(freq_sum / file_size)
        
        # String features
        try:
            content_str = content.decode('utf-8', errors='ignore')
            
            # String length ratio
            features.append(len(content_str) / max(1, file_size))
            
            # URL count
            url_count = len(re.findall(r'https?://[^\s]+', content_str))
            features.append(min(1.0, url_count / 10))
            
            # Base64 pattern count
            b64_patterns = len(re.findall(r'[A-Za-z0-9+/]{20,}={0,2}', content_str))
            features.append(min(1.0, b64_patterns / 5))
            
        except:
            # Add zeros for string features if decoding fails
            features.extend([0.0, 0.0, 0.0])
        
        # Entropy feature
        if file_size > 0:
            entropy = self._calculate_entropy(content)
            features.append(entropy / 8.0)  # Normalized
        
        return features
    
    def _calculate_entropy(self, data: bytes) -> float:
        """
        Calculate Shannon entropy of data
        """
        if len(data) == 0:
            return 0.0
        
        byte_counts = [0] * 256
        for byte in data:
            byte_counts[byte] += 1
        
        entropy = 0.0
        total_bytes = len(data)
        
        for count in byte_counts:
            if count > 0:
                probability = count / total_bytes
                entropy -= probability * np.log2(probability)
        
        return entropy
    
    def _update_avg_time(self, processing_time: float):
        """
        Update average processing time (exponential moving average)
        """
        alpha = 0.1
        self.metrics['avg_analysis_time'] = (
            alpha * processing_time + 
            (1 - alpha) * self.metrics['avg_analysis_time']
        )
    
    def _get_recommended_action(self, is_malware: bool, confidence: float, risk_level: str) -> str:
        """
        Get recommended action based on malware detection
        """
        if not is_malware:
            if risk_level == 'high_risk':
                return "QUARANTINE - High risk file type, needs manual review"
            elif risk_level == 'medium_risk':
                return "SCAN - Medium risk file, run additional scans"
            else:
                return "ALLOW - Low risk file"
        
        if confidence > 0.9:
            return "BLOCK - High confidence malware detected"
        
        if confidence > 0.7:
            return "QUARANTINE - Suspected malware, isolate for analysis"
        
        return "ALERT - Possible malware, notify security team"
    
    def _requires_sandbox_analysis(self, detections: List[Dict], file_type: str) -> bool:
        """
        Determine if file requires sandbox analysis
        
        Sandbox analysis runs file in isolated environment
        to observe behavior
        """
        # High confidence detections don't need sandbox
        high_conf_detections = [d for d in detections if d.get('confidence', 0) > 0.8]
        if high_conf_detections:
            return False
        
        # Executable files should go to sandbox
        if file_type in ['exe', 'elf', 'dll', 'so']:
            return True
        
        # Script files with medium confidence detections
        if file_type in ['php', 'js', 'py', 'rb', 'pl']:
            medium_conf_detections = [d for d in detections if d.get('confidence', 0) > 0.5]
            if medium_conf_detections:
                return True
        
        return False
    
    def _error_response(self, error_message: str) -> Dict[str, Any]:
        """
        Generate error response
        """
        return {
            'agent_id': self.agent_id,
            'agent_name': self.name,
            'error': error_message,
            'is_malware': False,
            'malware_confidence': 0.1,
            'malware_type': 'unknown',
            'recommended_action': 'INVESTIGATE - Malware analysis failed',
            'reasoning_state': self.get_reasoning_state(),
            'decision': {
                'is_malware': False,
                'confidence': 0.1,
                'malware_type': 'unknown',
                'evidence': [{'type': 'AGENT_ERROR', 'description': error_message}]
            }
        }
    
    def analyze_in_sandbox(self, content: bytes, filename: str) -> Dict[str, Any]:
        """
        Analyze file in sandbox environment
        
        Sandbox provides:
        - Behavioral analysis
        - Network activity monitoring
        - File system changes
        - Process execution tracking
        """
        # In production, integrate with sandbox solution
        # For now, return mock results
        
        sandbox_results = {
            'sandbox_id': f"sbx_{hashlib.md5(content).hexdigest()[:8]}",
            'execution_time': 30,
            'behaviors': [],
            'network_activity': [],
            'file_changes': [],
            'processes': [],
            'risk_score': 0.0,
            'verdict': 'unknown'
        }
        
        # Mock behaviors based on file type
        file_type = self._detect_file_type(content, filename)
        
        if file_type == 'php':
            sandbox_results['behaviors'] = [
                'Attempted to write to /tmp directory',
                'Made HTTP request to external IP',
                'Attempted to execute system command'
            ]
            sandbox_results['risk_score'] = 0.8
            sandbox_results['verdict'] = 'malicious'
        
        elif file_type == 'exe':
            sandbox_results['behaviors'] = [
                'Created registry entries',
                'Opened network sockets',
                'Attempted to download additional files'
            ]
            sandbox_results['risk_score'] = 0.9
            sandbox_results['verdict'] = 'malicious'
        
        else:
            sandbox_results['behaviors'] = ['No suspicious behavior detected']
            sandbox_results['risk_score'] = 0.1
            sandbox_results['verdict'] = 'clean'
        
        return sandbox_results
    
    def add_malware_hash(self, file_hash: str, malware_type: str = 'unknown'):
        """
        Add malware hash to database
        """
        self.malware_hashes.add(file_hash)
        
        # In production, save to database
        print(f"✅ Added malware hash: {file_hash[:16]}... ({malware_type})")
    
    def get_agent_status(self) -> Dict[str, Any]:
        """
        Get comprehensive agent status
        """
        return {
            'agent_id': self.agent_id,
            'name': self.name,
            'status': 'ACTIVE',
            'confidence': self.confidence,
            'metrics': self.metrics,
            'detection_capabilities': {
                'yara_rules': len(self.yara_rules),
                'malware_signatures': sum(len(sigs) for sigs in self.malware_signatures.values()),
                'heuristic_patterns': sum(len(patterns) for patterns in self.heuristic_patterns.values()),
                'known_malware_hashes': len(self.malware_hashes)
            },
            'config': {
                'thresholds': self.thresholds,
                'max_file_size': self.thresholds['max_file_size'],
                'file_type_detection': list(self.file_signatures.keys())
            }
        }