# config/mhc_config.yaml
# ============================================================================
# MANIFOLD-CONSTRAINED HYPER-CONNECTIONS (MHC) CONFIGURATION
# ============================================================================
# Configuration for the MHC architecture that stabilizes multi-agent reasoning
# through manifold constraints, preventing signal explosion and reasoning collapse
# ============================================================================

# ----------------------------------------------------------------------------
# MHC CORE ARCHITECTURE
# ----------------------------------------------------------------------------
mhc_core:
  # Architecture version
  version: "1.0.0"
  
  # Implementation type
  implementation: "torch"  # torch, tensorflow, numpy
  
  # Precision settings
  precision: "float32"  # float16, float32, bfloat16
  
  # Device configuration
  device: "cuda"  # cuda, cpu, auto
  
  # Parallel processing
  parallel_processing:
    enabled: true
    num_workers: 4
    backend: "torch"  # torch, ray, dask

# ----------------------------------------------------------------------------
# MANIFOLD CONFIGURATION
# ----------------------------------------------------------------------------
manifold:
  # Manifold type
  type: "learned"  # learned, fixed, adaptive
  
  # Dimensionality
  dimensionality:
    state_dim: 512
    latent_dim: 256
    embedding_dim: 128
    
  # Manifold constraints
  constraints:
    # Geometric constraints
    curvature: "non_positive"  # positive, negative, zero, non_positive
    geodesic_preservation: true
    volume_preservation: false
    
    # Topological constraints
    connectivity: "fully_connected"  # sparse, hierarchical, fully_connected
    sparsity_level: 0.1  # Only for sparse connectivity
    
    # Metric constraints
    metric: "learned"  # euclidean, hyperbolic, learned
    distance_function: "cosine"  # euclidean, cosine, mahalanobis
    
  # Manifold learning
  learning:
    # Learning algorithm
    algorithm: "autoencoder"  # autoencoder, variational, contrastive
    
    # Training parameters
    learning_rate: 0.001
    batch_size: 32
    epochs: 100
    early_stopping_patience: 10
    
    # Regularization
    regularization:
      l1_weight: 0.001
      l2_weight: 0.01
      manifold_smoothness: 0.1
      
    # Loss functions
    losses:
      reconstruction: 1.0
      manifold: 0.5
      contrastive: 0.3
      topological: 0.2

# ----------------------------------------------------------------------------
# HYPER-CONNECTION CONFIGURATION
# ----------------------------------------------------------------------------
hyper_connections:
  # Connection types
  types:
    - "residual"
    - "attention"
    - "gating"
    - "skip"
    
  # Connection strengths
  strengths:
    residual: 0.8
    attention: 0.6
    gating: 0.4
    skip: 0.2
    
  # Connection learning
  learning:
    # Adaptive connection strengths
    adaptive: true
    learning_rate: 0.01
    momentum: 0.9
    
    # Connection pruning
    pruning:
      enabled: true
      method: "magnitude"  # magnitude, gradient, random
      threshold: 0.01
      frequency: 1000  # steps
      
    # Connection growing
    growing:
      enabled: true
      method: "gradient"  # gradient, random, hebbian
      threshold: 0.1
      frequency: 1000  # steps
      
  # Connection patterns
  patterns:
    # Fully connected
    fully_connected:
      enabled: true
      symmetric: true
      self_connections: true
      
    # Hierarchical connections
    hierarchical:
      enabled: false
      levels: 3
      intra_level: 0.8
      inter_level: 0.2
      
    # Modular connections
    modular:
      enabled: false
      modules: 4
      intra_module: 0.9
      inter_module: 0.1

# ----------------------------------------------------------------------------
# DOUBLY-STOCHASTIC NORMALIZATION
# ----------------------------------------------------------------------------
doubly_stochastic:
  # Normalization method
  method: "sinkhorn_knopp"  # sinkhorn_knopp, iterative, closed_form
  
  # Sinkhorn-Knopp parameters
  sinkhorn:
    iterations: 50
    epsilon: 1e-8
    temperature: 1.0
    
    # Convergence criteria
    convergence:
      threshold: 1e-6
      check_frequency: 10
      max_iterations: 1000
      
    # Numerical stability
    stability:
      log_domain: true
      scaling: true
      clamping: true
      clamp_value: 1e-10
      
  # Alternative methods
  alternatives:
    iterative:
      enabled: false
      iterations: 100
      learning_rate: 0.1
      
    closed_form:
      enabled: false
      regularization: 0.01
      
  # Properties to enforce
  properties:
    row_stochastic: true
    column_stochastic: true
    non_negative: true
    symmetric: false
    
  # Regularization
  regularization:
    entropy: 0.1  # For smoother distributions
    sparsity: 0.01  # For sparser connections

# ----------------------------------------------------------------------------
# CONVEX STATE MIXING
# ----------------------------------------------------------------------------
convex_mixing:
  # Mixing strategy
  strategy: "weighted_average"  # weighted_average, attention, gated
  
  # Weight computation
  weights:
    method: "learned"  # learned, attention, similarity
    normalization: "softmax"  # softmax, sparsemax, sigmoid
    
    # Attention-based weights
    attention:
      heads: 8
      dropout: 0.1
      scale: true
      
    # Similarity-based weights
    similarity:
      metric: "cosine"  # cosine, euclidean, dot
      temperature: 0.07
      
  # Mixing coefficients
  coefficients:
    # Convex combination (sum to 1)
    convex: true
    non_negative: true
    sum_to_one: true
    
    # Bounds on coefficients
    bounds:
      min: 0.0
      max: 1.0
      epsilon: 1e-8
      
  # State representation
  state_representation:
    # Representation type
    type: "vector"  # vector, matrix, tensor
    
    # Dimensionality
    dimensions:
      batch: "dynamic"
      agents: "dynamic"
      features: 512
      time: "optional"
      
    # Normalization
    normalization:
      layer_norm: true
      batch_norm: false
      instance_norm: false
      
  # Mixing operation
  operation:
    # Linear mixing
    linear: true
    bias: true
    
    # Non-linear mixing
    non_linear:
      enabled: true
      activation: "gelu"  # relu, gelu, tanh, sigmoid
      dropout: 0.1
      
    # Residual connection
    residual:
      enabled: true
      scaling: 0.1

# ----------------------------------------------------------------------------
# IDENTITY-PRESERVING MAPPINGS
# ----------------------------------------------------------------------------
identity_preservation:
  # Preservation methods
  methods:
    - "residual_connection"
    - "layer_norm"
    - "skip_connection"
    - "attention_residual"
    
  # Residual connections
  residual:
    scaling_factor: 0.1
    learnable_scaling: true
    conditional_scaling: true
    
  # Layer normalization
  layer_norm:
    epsilon: 1e-5
    elementwise_affine: true
    learnable_parameters: true
    
  # Skip connections
  skip:
    frequency: "every_layer"  # every_layer, alternate, custom
    connection_type: "add"  # add, concatenate, gate
    
  # Identity loss
  loss:
    enabled: true
    weight: 0.01
    type: "mse"  # mse, cosine, l1
    
    # Target identity
    target: "input"  # input, previous, learned
    
    # Regularization
    regularization: 0.001
    
  # Identity verification
  verification:
    enabled: true
    threshold: 0.9  # Minimum similarity to identity
    frequency: 100  # steps
    
    # Metrics
    metrics:
      - "cosine_similarity"
      - "l2_distance"
      - "rank_correlation"

# ----------------------------------------------------------------------------
# NON-EXPANSIVE UPDATES
# ----------------------------------------------------------------------------
non_expansive:
  # Contraction methods
  methods:
    - "lipschitz_constraint"
    - "gradient_clipping"
    - "weight_normalization"
    - "spectral_normalization"
    
  # Lipschitz constraints
  lipschitz:
    enabled: true
    constant: 1.0
    enforce_strictly: true
    
    # Estimation method
    estimation: "power_iteration"  # power_iteration, spectral, lipschitz_net
    
    # Power iteration
    power_iteration:
      steps: 1
      tolerance: 1e-6
      
  # Gradient clipping
  gradient_clipping:
    enabled: true
    method: "norm"  # norm, value, adaptive
    
    # Norm clipping
    norm_clipping:
      max_norm: 1.0
      norm_type: 2.0
      
    # Value clipping
    value_clipping:
      min_value: -1.0
      max_value: 1.0
      
  # Weight normalization
  weight_norm:
    enabled: true
    type: "spectral"  # spectral, frobenius, infinity
    
    # Spectral normalization
    spectral_norm:
      power_iterations: 1
      eps: 1e-12
      
  # Stability monitoring
  stability_monitoring:
    enabled: true
    metrics:
      - "gradient_norm"
      - "weight_norm"
      - "activation_norm"
      - "condition_number"
      
    # Alert thresholds
    alerts:
      gradient_explosion: 100.0
      gradient_vanishing: 1e-6
      weight_explosion: 10.0
      
  # Stability guarantees
  guarantees:
    # Theoretical guarantees
    theoretical: true
    proof_sketch: "Based on contraction mapping theorem"
    
    # Empirical validation
    empirical:
      enabled: true
      test_cases: 1000
      max_iterations: 10000

# ----------------------------------------------------------------------------
# SIGNAL BOUNDING
# ----------------------------------------------------------------------------
signal_bounding:
  # Bounding methods
  methods:
    - "norm_clipping"
    - "value_clipping"
    - "saturation"
    - "normalization"
    
  # Norm-based bounding
  norm_bounding:
    enabled: true
    max_norm: 1.0
    norm_type: 2.0  # 1, 2, inf
    
    # Adaptive bounding
    adaptive: true
    learning_rate: 0.01
    momentum: 0.9
    
  # Value-based bounding
  value_bounding:
    enabled: true
    min_value: -1.0
    max_value: 1.0
    
    # Per-channel bounds
    per_channel: true
    learnable_bounds: true
    
  # Saturation functions
  saturation:
    enabled: true
    function: "tanh"  # tanh, sigmoid, hard_tanh, softsign
    
    # Tanh parameters
    tanh:
      scale: 1.0
      learnable_scale: true
      
  # Normalization methods
  normalization:
    enabled: true
    methods:
      - "batch_norm"
      - "layer_norm"
      - "instance_norm"
      - "group_norm"
      
    # Batch normalization
    batch_norm:
      momentum: 0.1
      eps: 1e-5
      affine: true
      
  # Signal monitoring
  monitoring:
    enabled: true
    metrics:
      - "signal_mean"
      - "signal_std"
      - "signal_range"
      - "signal_entropy"
      
    # Histogram tracking
    histograms:
      enabled: true
      bins: 100
      range: [-3, 3]
      
    # Alerting
    alerts:
      signal_explosion: "std > 10.0"
      signal_collapse: "std < 0.01"
      signal_bias: "|mean| > 1.0"

# ----------------------------------------------------------------------------
# AGENT COORDINATION VIA MHC
# ----------------------------------------------------------------------------
agent_coordination:
  # Coordination protocol
  protocol: "mhc_based"  # mhc_based, voting, consensus, hierarchical
  
  # Message passing
  message_passing:
    enabled: true
    rounds: 3
    aggregation: "mean"  # mean, max, attention, learned
    
    # Message types
    message_types:
      - "state_update"
      - "confidence_score"
      - "attention_weights"
      - "gradient_information"
      
    # Message compression
    compression:
      enabled: true
      method: "autoencoder"  # autoencoder, pca, random_projection
      compression_ratio: 0.5
      
  # Attention mechanism
  attention:
    enabled: true
    type: "multi_head"  # multi_head, gqa, linear
    heads: 8
    dropout: 0.1
    
    # Query, Key, Value projections
    projections:
      q_dim: 64
      k_dim: 64
      v_dim: 64
      output_dim: 512
      
    # Attention computation
    computation:
      scale: true
      causal: false
      softmax_temperature: 1.0
      
  # Consensus building
  consensus:
    enabled: true
    method: "weighted_average"  # weighted_average, majority_vote, bayesian
    
    # Weight computation
    weights:
      based_on: "confidence"  # confidence, accuracy, similarity
      normalization: "softmax"
      temperature: 1.0
      
    # Convergence criteria
    convergence:
      threshold: 0.01
      max_iterations: 10
      check_frequency: 1

# ----------------------------------------------------------------------------
# RESIDUAL COORDINATION
# ----------------------------------------------------------------------------
residual_coordination:
  # Residual computation
  computation:
    input_dim: 512
    hidden_dim: 1024
    output_dim: 512
    num_layers: 2
    
  # Residual blocks
  blocks:
    - type: "linear"
      activation: "gelu"
      dropout: 0.1
      normalization: "layer_norm"
      
    - type: "attention"
      heads: 8
      dropout: 0.1
      residual: true
      
  # Skip connections
  skip_connections:
    enabled: true
    pattern: "dense"  # dense, residual, dense
    scaling: "learned"  # fixed, learned, conditional
    
  # Gradient flow
  gradient_flow:
    # Gradient preservation
    preservation: true
    methods:
      - "residual_connections"
      - "layer_norm"
      - "gradient_checkpointing"
      
    # Gradient monitoring
    monitoring:
      enabled: true
      metrics:
        - "gradient_norm"
        - "gradient_std"
        - "gradient_mean"
        
  # State propagation
  state_propagation:
    # Propagation rules
    rules: "learned"  # fixed, learned, attention_based
    
    # Learned propagation
    learned:
      network_type: "transformer"  # transformer, mlp, rnn
      num_layers: 3
      hidden_dim: 512
      
    # Fixed propagation
    fixed:
      matrix: "doubly_stochastic"
      sparsity: 0.1

# ----------------------------------------------------------------------------
# PERFORMANCE OPTIMIZATION
# ----------------------------------------------------------------------------
performance:
  # Computational optimizations
  computational:
    # Mixed precision
    mixed_precision: true
    precision: "float16"  # float16, bfloat16
    
    # Gradient checkpointing
    gradient_checkpointing: true
    checkpoint_every: 2
    
    # Memory optimization
    memory:
      enable_optimization: true
      optimization_level: "O2"  # O0, O1, O2, O3
      
  # Parallel processing
  parallel:
    # Data parallelism
    data_parallel: true
    num_devices: "auto"
    
    # Model parallelism
    model_parallel: false
    pipeline_parallel: false
    tensor_parallel: false
    
    # Distributed training
    distributed:
      enabled: false
      backend: "nccl"  # nccl, gloo, mpi
      init_method: "env://"
      
  # Caching strategies
  caching:
    # Attention caching
    attention_cache: true
    cache_size: 1000
    
    # State caching
    state_cache: true
    cache_ttl: 3600  # seconds
    
    # Result caching
    result_cache: true
    cache_key: "hash(input)"
    
  # Batch processing
  batching:
    enabled: true
    batch_size: 32
    dynamic_batching: true
    max_batch_size: 128
    
    # Padding and masking
    padding:
      enabled: true
      value: 0
      side: "right"  # left, right
      
    # Bucketing
    bucketing:
      enabled: true
      bucket_boundaries: [32, 64, 128, 256]
      
  # Latency optimization
  latency:
    # Asynchronous processing
    async_processing: true
    queue_size: 1000
    
    # Prefetching
    prefetch: true
    prefetch_factor: 2
    
    # Early stopping
    early_stop:
      enabled: true
      confidence_threshold: 0.95
      max_depth: 3

# ----------------------------------------------------------------------------
# MONITORING & DIAGNOSTICS
# ----------------------------------------------------------------------------
monitoring:
  # Metrics collection
  metrics:
    enabled: true
    collection_interval: 60  # seconds
    
    # MHC-specific metrics
    mhc_metrics:
      - "manifold_curvature"
      - "connection_strength"
      - "doubly_stochastic_error"
      - "identity_preservation_error"
      - "non_expansive_violation"
      - "signal_bound_violation"
      
    # Performance metrics
    performance_metrics:
      - "inference_time"
      - "memory_usage"
      - "cpu_usage"
      - "gpu_usage"
      - "batch_processing_time"
      
  # Visualization
  visualization:
    enabled: true
    tools:
      - "tensorboard"
      - "wandb"
      - "mlflow"
      
    # What to visualize
    visualizations:
      - "manifold_projection"
      - "connection_graph"
      - "attention_heatmaps"
      - "gradient_flow"
      - "signal_distribution"
      
  # Debugging
  debugging:
    enabled: true
    log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR
    
    # Debug checks
    checks:
      - "nan_detection"
      - "inf_detection"
      - "gradient_check"
      - "parameter_check"
      
    # Assertions
    assertions:
      enabled: true
      check_frequency: 100  # steps
      
      # MHC assertions
      mhc_assertions:
        - "doubly_stochastic"
        - "non_negative"
        - "identity_preservation"
        - "signal_bounded"
        
  # Health checks
  health_checks:
    enabled: true
    interval: 300  # seconds
    
    # Check types
    checks:
      - "memory_leak"
      - "deadlock"
      - "resource_exhaustion"
      - "performance_degradation"
      
    # Auto-remediation
    auto_remediation:
      enabled: true
      actions:
        - "restart_process"
        - "clear_cache"
        - "reduce_load"
        - "switch_to_backup"

# ----------------------------------------------------------------------------
# TRAINING CONFIGURATION
# ----------------------------------------------------------------------------
training:
  # Training data
  data:
    # Dataset configuration
    datasets:
      - name: "manifold_learning"
        path: "./data/manifold/"
        split: [0.7, 0.15, 0.15]  # train, val, test
        
      - name: "coordination_examples"
        path: "./data/coordination/"
        split: [0.8, 0.1, 0.1]
        
    # Data augmentation
    augmentation:
      enabled: true
      methods:
        - "noise_injection"
        - "random_projection"
        - "mixup"
        - "cutmix"
        
  # Training loop
  loop:
    # Optimizer
    optimizer:
      type: "adamw"
      learning_rate: 0.001
      weight_decay: 0.01
      betas: [0.9, 0.999]
      eps: 1e-8
      
    # Scheduler
    scheduler:
      type: "cosine"
      warmup_steps: 1000
      total_steps: 100000
      min_lr: 1e-6
      
    # Loss functions
    losses:
      reconstruction: 1.0
      manifold: 0.5
      coordination: 0.3
      regularization: 0.1
      
  # Validation
  validation:
    enabled: true
    frequency: 1000  # steps
    
    # Validation metrics
    metrics:
      - "reconstruction_error"
      - "manifold_quality"
      - "coordination_efficiency"
      - "stability_score"
      
    # Early stopping
    early_stopping:
      enabled: true
      patience: 10
      min_delta: 0.001
      
  # Checkpointing
  checkpointing:
    enabled: true
    frequency: 5000  # steps
    save_best: true
    
    # Checkpoint format
    format: "torch"  # torch, onnx, safetensors
    
    # Compression
    compression: true
    algorithm: "gzip"

# ----------------------------------------------------------------------------
# DEPLOYMENT CONFIGURATION
# ----------------------------------------------------------------------------
deployment:
  # Model formats
  formats:
    inference: "torchscript"  # torchscript, onnx, tensorrt
    serving: "torchserve"  # torchserve, triton, custom
    
  # Serving configuration
  serving:
    # TorchServe
    torchserve:
      model_store: "./models/"
      inference_address: "0.0.0.0:8080"
      management_address: "0.0.0.0:8081"
      
    # Triton Inference Server
    triton:
      model_repository: "./models/"
      http_port: 8000
      grpc_port: 8001
      metrics_port: 8002
      
  # Scaling configuration
  scaling:
    # Horizontal scaling
    horizontal:
      enabled: true
      min_replicas: 1
      max_replicas: 10
      
      # Auto-scaling
      auto_scaling:
        enabled: true
        target_cpu_utilization: 70
        target_memory_utilization: 80
        
    # Vertical scaling
    vertical:
      enabled: false
      resources:
        cpu: "2"
        memory: "4Gi"
        gpu: "1"
        
  # Monitoring endpoints
  endpoints:
    health: "/health"
    metrics: "/metrics"
    ready: "/ready"
    live: "/live"
    
  # Security
  security:
    authentication:
      enabled: true
      method: "jwt"  # jwt, api_key, mTLS
      
    encryption:
      enabled: true
      tls: true
      https_only: true

# ============================================================================
# END OF MHC CONFIGURATION
# ============================================================================