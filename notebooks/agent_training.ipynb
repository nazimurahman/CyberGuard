{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1115208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CyberGuard REST API Server for production deployment.\n",
    "This server provides a secure, scalable API for security threat analysis.\n",
    "\"\"\"\n",
    "# Documentation string: Describes the purpose of this module.\n",
    "# This API serves as the deployment interface for the trained CyberGuard model.\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS AND DEPENDENCIES\n",
    "# ============================================================================\n",
    "\n",
    "# Standard library imports (Python built-in modules)\n",
    "import json  # For parsing and generating JSON data\n",
    "import yaml  # For parsing YAML configuration files\n",
    "import logging  # For logging application events and errors\n",
    "from pathlib import Path  # For object-oriented filesystem path manipulation\n",
    "from typing import Dict, List, Optional, Any  # Type hints for better code documentation\n",
    "from datetime import datetime  # For handling timestamps and dates\n",
    "from functools import lru_cache  # For caching function results to improve performance\n",
    "\n",
    "# Core machine learning dependencies\n",
    "import torch  # PyTorch: Main deep learning framework for model inference\n",
    "import numpy as np  # NumPy: Numerical computing library for array operations\n",
    "\n",
    "# FastAPI framework imports for building the web server\n",
    "from fastapi import FastAPI, HTTPException, Depends, Security, Request\n",
    "# FastAPI: Modern web framework for building APIs\n",
    "# HTTPException: For raising HTTP errors with specific status codes\n",
    "# Depends: Dependency injection system for request handling\n",
    "# Security: For implementing security mechanisms\n",
    "# Request: For accessing request metadata\n",
    "\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "# HTTPBearer: Security scheme for Bearer token authentication\n",
    "# HTTPAuthorizationCredentials: Type for storing authentication credentials\n",
    "\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "# CORSMiddleware: Middleware for handling Cross-Origin Resource Sharing\n",
    "\n",
    "from fastapi.middleware.trustedhost import TrustedHostMiddleware\n",
    "# TrustedHostMiddleware: Security middleware to validate host headers\n",
    "\n",
    "from pydantic import BaseModel, Field, validator\n",
    "# Pydantic: Data validation and settings management\n",
    "# BaseModel: Base class for creating data models with validation\n",
    "# Field: For defining field metadata and constraints\n",
    "# validator: Decorator for custom validation functions\n",
    "\n",
    "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
    "# SlowAPI: Rate limiting extension for FastAPI\n",
    "# Limiter: Main rate limiting class\n",
    "# _rate_limit_exceeded_handler: Default handler for rate limit violations\n",
    "\n",
    "from slowapi.util import get_remote_address\n",
    "# get_remote_address: Function to extract client IP address for rate limiting\n",
    "\n",
    "from slowapi.errors import RateLimitExceeded\n",
    "# RateLimitExceeded: Exception raised when rate limit is exceeded\n",
    "\n",
    "# Conditional imports for optional dependencies with fallback handling\n",
    "try:\n",
    "    import onnxruntime as ort  # ONNX Runtime for model inference optimization\n",
    "    ONNX_AVAILABLE = True  # Flag indicating ONNX Runtime is available\n",
    "except ImportError:\n",
    "    ONNX_AVAILABLE = False  # ONNX Runtime not available\n",
    "    ort = None  # Set to None to avoid NameError\n",
    "\n",
    "try:\n",
    "    import tensorrt  # NVIDIA TensorRT for GPU acceleration (optional)\n",
    "    TENSORRT_AVAILABLE = True  # Flag indicating TensorRT is available\n",
    "except ImportError:\n",
    "    TENSORRT_AVAILABLE = False  # TensorRT not available\n",
    "    tensorrt = None  # Set to None to avoid NameError\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: LOGGING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Configure logging for production use\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set minimum log level to INFO (captures INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    # Log format: timestamp - logger name - log level - message\n",
    "    handlers=[\n",
    "        logging.FileHandler('cyberguard_api.log'),  # Write logs to file\n",
    "        logging.StreamHandler()  # Also output logs to console (stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create logger instance for this module\n",
    "logger = logging.getLogger(__name__)\n",
    "# __name__ gives the module's name ('__main__' if run directly, otherwise module path)\n",
    "# This allows hierarchical logging configuration\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: SECURITY SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Create HTTP Bearer authentication security scheme\n",
    "security = HTTPBearer(auto_error=False)\n",
    "# auto_error=False: Don't automatically raise error if credentials missing\n",
    "# This allows endpoints to handle missing credentials gracefully\n",
    "\n",
    "# Initialize rate limiter using client IP address as the key\n",
    "limiter = Limiter(key_func=get_remote_address)\n",
    "# key_func: Function that returns a unique identifier for rate limiting\n",
    "# get_remote_address: Extracts client IP from request\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: DATA MODELS (Pydantic Schemas)\n",
    "# ============================================================================\n",
    "\n",
    "class AnalysisRequest(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model for validating analysis request payload.\n",
    "    Ensures incoming data meets expected format and constraints.\n",
    "    \"\"\"\n",
    "    text: str = Field(\n",
    "        ...,  # Ellipsis indicates this field is required (cannot be None)\n",
    "        description=\"Text to analyze for security threats. This can be HTTP requests, logs, or any security-relevant text.\",\n",
    "        min_length=1,  # Minimum string length validation\n",
    "        max_length=10000  # Maximum string length to prevent overly large requests\n",
    "    )\n",
    "    # text field: The input text to be analyzed for security threats\n",
    "    \n",
    "    detailed: bool = Field(\n",
    "        False,  # Default value if not provided\n",
    "        description=\"If True, returns detailed analysis including probabilities for all threat categories.\"\n",
    "    )\n",
    "    # detailed field: Flag controlling the verbosity of the response\n",
    "    \n",
    "    @validator('text')  # Decorator: This function validates the 'text' field\n",
    "    def validate_text_not_empty(cls, v):  # cls: class reference, v: value to validate\n",
    "        \"\"\"Ensure text is not just whitespace\"\"\"\n",
    "        # v.strip(): Remove leading/trailing whitespace\n",
    "        # not v: Check if string is empty\n",
    "        # not v.strip(): Check if string contains only whitespace\n",
    "        if not v or not v.strip():\n",
    "            raise ValueError('Text cannot be empty or whitespace only')  # Raise validation error\n",
    "        return v.strip()  # Return stripped version of the text\n",
    "\n",
    "\n",
    "class AnalysisResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model for standardizing API response format.\n",
    "    Ensures consistent response structure for clients.\n",
    "    \"\"\"\n",
    "    success: bool = Field(..., description=\"Whether the analysis was successful\")\n",
    "    # success field: Boolean indicating if the request was processed successfully\n",
    "    \n",
    "    analysis: Dict[str, Any] = Field(..., description=\"Analysis results\")\n",
    "    # analysis field: Dictionary containing the actual analysis results\n",
    "    # Dict[str, Any]: Keys are strings, values can be any type\n",
    "    \n",
    "    timestamp: str = Field(..., description=\"ISO format timestamp of analysis\")\n",
    "    # timestamp field: String containing when the analysis was performed\n",
    "    \n",
    "    model_version: str = Field(..., description=\"Version of the model used\")\n",
    "    # model_version field: String indicating which model version was used\n",
    "    \n",
    "    request_id: Optional[str] = Field(None, description=\"Unique ID for tracking this request\")\n",
    "    # request_id field: Optional unique identifier for request tracking\n",
    "    # Optional[str]: Can be string or None\n",
    "    # None: Default value if not provided\n",
    "\n",
    "\n",
    "class ThreatCategory(BaseModel):\n",
    "    \"\"\"\n",
    "    Model representing a threat category with metadata.\n",
    "    Used to structure threat category information.\n",
    "    \"\"\"\n",
    "    id: int = Field(..., description=\"Category ID\")  # Unique numerical identifier\n",
    "    name: str = Field(..., description=\"Category name\")  # Human-readable name\n",
    "    description: str = Field(..., description=\"Category description\")  # Detailed description\n",
    "    severity_weight: float = Field(..., description=\"Weight for severity calculation\")\n",
    "    # severity_weight: Numerical weight used in severity calculations (0.0 to 1.0)\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: MAIN API HANDLER CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class CyberGuardAPI:\n",
    "    \"\"\"\n",
    "    Main API handler class that manages model loading, inference, and request processing.\n",
    "    This class follows the singleton pattern for efficient resource management.\n",
    "    Singleton pattern ensures only one instance exists, preventing redundant model loading.\n",
    "    \"\"\"\n",
    "    \n",
    "    _instance = None  # Class variable to store the single instance (singleton pattern)\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Override __new__ to implement singleton pattern.\n",
    "        Called before __init__ during object creation.\n",
    "        \"\"\"\n",
    "        if cls._instance is None:  # Check if instance doesn't exist\n",
    "            # Create new instance by calling parent class's __new__ method\n",
    "            cls._instance = super(CyberGuardAPI, cls).__new__(cls)\n",
    "        return cls._instance  # Always return the same instance\n",
    "    \n",
    "    def __init__(self, model_dir: Path, config_path: Path, tokenizer_path: Path):\n",
    "        \"\"\"\n",
    "        Initialize the API with model, configuration, and tokenizer.\n",
    "        Only executes once due to singleton pattern.\n",
    "        \n",
    "        Args:\n",
    "            model_dir: Directory containing model files (TorchScript, ONNX, PyTorch)\n",
    "            config_path: Path to configuration YAML file with model settings\n",
    "            tokenizer_path: Path to tokenizer JSON file for text processing\n",
    "        \"\"\"\n",
    "        # Skip reinitialization if already initialized (singleton safeguard)\n",
    "        if hasattr(self, 'initialized') and self.initialized:\n",
    "            return  # Exit early if already initialized\n",
    "            \n",
    "        # Store paths as instance variables\n",
    "        self.model_dir = model_dir  # Directory where model files are stored\n",
    "        self.config_path = config_path  # Path to configuration file\n",
    "        self.tokenizer_path = tokenizer_path  # Path to tokenizer vocabulary file\n",
    "        \n",
    "        # Load and validate configuration from YAML file\n",
    "        self.config = self._load_and_validate_config()\n",
    "        # self.config: Dictionary containing all configuration settings\n",
    "        \n",
    "        # Load tokenizer and create reverse lookup dictionary\n",
    "        self.tokenizer, self.reverse_tokenizer = self._load_tokenizer()\n",
    "        # self.tokenizer: Dictionary mapping tokens to IDs\n",
    "        # self.reverse_tokenizer: Dictionary mapping IDs to tokens (for debugging)\n",
    "        \n",
    "        # Determine the best available computation device (GPU/CPU)\n",
    "        self.device = self._get_device()\n",
    "        # self.device: torch.device object (e.g., 'cuda', 'cpu', 'mps')\n",
    "        \n",
    "        # Load threat categories from configuration\n",
    "        self.threat_categories = self._load_threat_categories()\n",
    "        # self.threat_categories: List of ThreatCategory objects\n",
    "        \n",
    "        # Load the trained model in the best available format\n",
    "        self.model = self._load_model()\n",
    "        # self.model: Loaded model ready for inference\n",
    "        \n",
    "        # Warm up model to avoid cold start latency (especially important for GPU)\n",
    "        self._warmup_model()\n",
    "        \n",
    "        # Load API keys for authentication\n",
    "        self.api_keys = self._load_api_keys()\n",
    "        # self.api_keys: Set of valid API keys\n",
    "        \n",
    "        # Initialize counters for statistics\n",
    "        self.request_count = 0  # Total number of requests processed\n",
    "        self.threat_detection_count = 0  # Number of malicious requests detected\n",
    "        \n",
    "        # Mark as initialized to prevent reinitialization\n",
    "        self.initialized = True\n",
    "        \n",
    "        # Log successful initialization\n",
    "        logger.info(f\"CyberGuard API initialized successfully on device: {self.device}\")\n",
    "        logger.info(f\"Model type: {type(self.model).__name__}\")  # Log model class name\n",
    "        logger.info(f\"Threat categories loaded: {len(self.threat_categories)}\")  # Log category count\n",
    "    \n",
    "    def _load_and_validate_config(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Load and validate configuration file.\n",
    "        \n",
    "        Returns:\n",
    "            Validated configuration dictionary\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If config file doesn't exist\n",
    "            yaml.YAMLError: If config file is invalid YAML\n",
    "            ValueError: If required config fields are missing\n",
    "        \"\"\"\n",
    "        # Check if configuration file exists\n",
    "        if not self.config_path.exists():\n",
    "            error_msg = f\"Configuration file not found: {self.config_path}\"\n",
    "            logger.error(error_msg)  # Log error\n",
    "            raise FileNotFoundError(error_msg)  # Raise exception\n",
    "        \n",
    "        try:\n",
    "            # Open and parse YAML file\n",
    "            with open(self.config_path, 'r') as f:  # 'r' mode for reading\n",
    "                config = yaml.safe_load(f)  # Parse YAML safely (no code execution)\n",
    "        except yaml.YAMLError as e:  # Catch YAML parsing errors\n",
    "            error_msg = f\"Invalid YAML in config file: {e}\"\n",
    "            logger.error(error_msg)\n",
    "            raise yaml.YAMLError(error_msg)  # Re-raise with context\n",
    "        \n",
    "        # Define required configuration fields\n",
    "        required_fields = ['model_name', 'model_version', 'max_seq_length', 'threat_categories']\n",
    "        \n",
    "        # Validate each required field exists in config\n",
    "        for field in required_fields:\n",
    "            if field not in config:  # Check if field is missing\n",
    "                error_msg = f\"Missing required config field: {field}\"\n",
    "                logger.error(error_msg)\n",
    "                raise ValueError(error_msg)  # Raise validation error\n",
    "        \n",
    "        logger.info(f\"Configuration loaded from: {self.config_path}\")\n",
    "        return config  # Return validated configuration\n",
    "    \n",
    "    def _load_tokenizer(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Load tokenizer from JSON file and create reverse mapping.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (token_to_id dictionary, id_to_token dictionary)\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If tokenizer file doesn't exist\n",
    "            json.JSONDecodeError: If tokenizer file is invalid JSON\n",
    "        \"\"\"\n",
    "        # Check if tokenizer file exists\n",
    "        if not self.tokenizer_path.exists():\n",
    "            error_msg = f\"Tokenizer file not found: {self.tokenizer_path}\"\n",
    "            logger.error(error_msg)\n",
    "            raise FileNotFoundError(error_msg)\n",
    "        \n",
    "        try:\n",
    "            # Open and parse JSON file\n",
    "            with open(self.tokenizer_path, 'r') as f:\n",
    "                tokenizer = json.load(f)  # Parse JSON to Python dictionary\n",
    "        except json.JSONDecodeError as e:  # Catch JSON parsing errors\n",
    "            error_msg = f\"Invalid JSON in tokenizer file: {e}\"\n",
    "            logger.error(error_msg)\n",
    "            raise json.JSONDecodeError(error_msg)\n",
    "        \n",
    "        # Create reverse mapping: ID -> Token (for debugging and interpretation)\n",
    "        reverse_tokenizer = {v: k for k, v in tokenizer.items()}\n",
    "        # Dictionary comprehension: Create new dict with swapped key-value pairs\n",
    "        \n",
    "        logger.info(f\"Tokenizer loaded with {len(tokenizer)} tokens\")\n",
    "        return tokenizer, reverse_tokenizer  # Return both mappings\n",
    "    \n",
    "    def _get_device(self) -> torch.device:\n",
    "        \"\"\"\n",
    "        Determine the best available device for computation.\n",
    "        Prioritizes CUDA GPU, then MPS (Apple Silicon), then CPU.\n",
    "        \n",
    "        Returns:\n",
    "            torch.device object specifying where computations should run\n",
    "        \"\"\"\n",
    "        # Check if CUDA (NVIDIA GPU) is available\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')  # Create CUDA device\n",
    "            # Get GPU information for logging\n",
    "            gpu_name = torch.cuda.get_device_name(0)  # Name of first GPU\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # Memory in GB\n",
    "            logger.info(f\"Using CUDA GPU: {gpu_name} ({gpu_memory:.2f} GB)\")\n",
    "        \n",
    "        # Check if MPS (Apple Silicon GPU) is available (macOS only)\n",
    "        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            device = torch.device('mps')  # Create MPS device (Metal Performance Shaders)\n",
    "            logger.info(\"Using Apple MPS (Metal Performance Shaders)\")\n",
    "        \n",
    "        # Fallback to CPU if no GPU available\n",
    "        else:\n",
    "            device = torch.device('cpu')  # Create CPU device\n",
    "            logger.info(\"Using CPU (no GPU available)\")\n",
    "        \n",
    "        return device  # Return the selected device\n",
    "    \n",
    "    def _load_threat_categories(self) -> List[ThreatCategory]:\n",
    "        \"\"\"\n",
    "        Load threat categories from configuration.\n",
    "        \n",
    "        Returns:\n",
    "            List of ThreatCategory objects\n",
    "        \"\"\"\n",
    "        categories = []  # Initialize empty list to store categories\n",
    "        \n",
    "        # Get threat categories list from config, default to empty list if not found\n",
    "        category_configs = self.config.get('threat_categories', [])\n",
    "        # .get() method: Returns default value if key doesn't exist\n",
    "        \n",
    "        # Enumerate through category configurations\n",
    "        # enumerate() returns (index, item) pairs\n",
    "        for i, cat_config in enumerate(category_configs):\n",
    "            # Create ThreatCategory object from config\n",
    "            category = ThreatCategory(\n",
    "                id=i,  # Use enumeration index as ID\n",
    "                name=cat_config.get('name', f'category_{i}'),  # Get name or use default\n",
    "                description=cat_config.get('description', ''),  # Get description or empty string\n",
    "                severity_weight=cat_config.get('severity_weight', 0.5)  # Get weight or default 0.5\n",
    "            )\n",
    "            categories.append(category)  # Add to list\n",
    "        \n",
    "        return categories  # Return complete list\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        Load the trained model in the best available format.\n",
    "        Attempts loading in this order: TorchScript -> ONNX -> PyTorch\n",
    "        \n",
    "        Returns:\n",
    "            Loaded model ready for inference\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: If no model file can be loaded\n",
    "        \"\"\"\n",
    "        # List of model formats to try, in order of preference\n",
    "        # Each tuple: (format_name, file_path)\n",
    "        model_files = [\n",
    "            ('TorchScript', self.model_dir / 'cyberguard.pt'),  # TorchScript format\n",
    "            ('ONNX', self.model_dir / 'cyberguard.onnx'),  # ONNX format\n",
    "            ('PyTorch', self.model_dir / 'cyberguard_optimized.pt'),  # Optimized PyTorch\n",
    "            ('PyTorch State Dict', self.model_dir / 'model_state_dict.pt')  # State dict only\n",
    "        ]\n",
    "        \n",
    "        # Variables to store loaded model and format\n",
    "        loaded_model = None  # Will hold the successfully loaded model\n",
    "        loaded_format = None  # Will hold the format name\n",
    "        \n",
    "        # Try each model format\n",
    "        for format_name, model_path in model_files:\n",
    "            if model_path.exists():  # Check if file exists\n",
    "                try:\n",
    "                    # --- TORCHSCRIPT LOADING ---\n",
    "                    if format_name == 'TorchScript':\n",
    "                        # Load TorchScript model\n",
    "                        loaded_model = torch.jit.load(model_path, map_location=self.device)\n",
    "                        # torch.jit.load: Loads TorchScript model\n",
    "                        # map_location: Specifies where to load the model (GPU/CPU)\n",
    "                        \n",
    "                        loaded_model.eval()  # Set model to evaluation mode\n",
    "                        # eval() mode: Disables dropout, batch norm uses running statistics\n",
    "                        \n",
    "                        loaded_format = 'TorchScript'  # Record format\n",
    "                        logger.info(f\"Successfully loaded TorchScript model from {model_path}\")\n",
    "                        break  # Exit loop since we found a working model\n",
    "                    \n",
    "                    # --- ONNX LOADING ---\n",
    "                    elif format_name == 'ONNX' and ONNX_AVAILABLE:\n",
    "                        # Create ONNX Runtime session options\n",
    "                        session_options = ort.SessionOptions()\n",
    "                        session_options.intra_op_num_threads = 1  # Threads for single operation\n",
    "                        session_options.inter_op_num_threads = 1  # Threads between operations\n",
    "                        \n",
    "                        # Set execution providers in order of preference\n",
    "                        providers = ['CPUExecutionProvider']  # Default to CPU\n",
    "                        if 'CUDAExecutionProvider' in ort.get_available_providers():\n",
    "                            # If CUDA provider available, use it first\n",
    "                            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "                        \n",
    "                        # Create ONNX Runtime inference session\n",
    "                        loaded_model = ort.InferenceSession(\n",
    "                            str(model_path),  # Convert Path to string\n",
    "                            sess_options=session_options,  # Session configuration\n",
    "                            providers=providers  # Execution providers\n",
    "                        )\n",
    "                        loaded_format = 'ONNX'\n",
    "                        logger.info(f\"Successfully loaded ONNX model from {model_path}\")\n",
    "                        break\n",
    "                    \n",
    "                    # --- PYTORCH LOADING ---\n",
    "                    elif format_name.startswith('PyTorch'):\n",
    "                        try:\n",
    "                            # Load PyTorch checkpoint\n",
    "                            checkpoint = torch.load(model_path, map_location=self.device)\n",
    "                            # torch.load: Loads PyTorch saved objects\n",
    "                            \n",
    "                            if 'model_state_dict' in checkpoint:\n",
    "                                # Checkpoint contains only state dict (weights)\n",
    "                                # Need model class definition to reconstruct\n",
    "                                logger.warning(\"PyTorch model loading requires model class definition\")\n",
    "                                continue  # Skip to next format\n",
    "                            else:\n",
    "                                # Assume it's a complete saved model\n",
    "                                loaded_model = checkpoint  # Use directly\n",
    "                                loaded_model.eval()  # Set to evaluation mode\n",
    "                                loaded_format = 'PyTorch'\n",
    "                                logger.info(f\"Successfully loaded PyTorch model from {model_path}\")\n",
    "                                break\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Failed to load {format_name}: {e}\")\n",
    "                            continue  # Try next format\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    # Catch any error during loading\n",
    "                    logger.warning(f\"Failed to load {format_name} model: {e}\")\n",
    "                    continue  # Try next format\n",
    "        \n",
    "        # Check if any model was successfully loaded\n",
    "        if loaded_model is None:\n",
    "            # Create error message listing all formats tried\n",
    "            error_msg = \"No model file could be loaded. Checked: \" + \", \".join([f[0] for f in model_files])\n",
    "            logger.error(error_msg)\n",
    "            raise RuntimeError(error_msg)  # Raise error if no model found\n",
    "        \n",
    "        # Store format information\n",
    "        self.model_format = loaded_format\n",
    "        return loaded_model  # Return the loaded model\n",
    "    \n",
    "    def _warmup_model(self):\n",
    "        \"\"\"\n",
    "        Warm up the model with dummy inference to avoid cold start latency.\n",
    "        This is especially important for GPU models to initialize CUDA context.\n",
    "        \"\"\"\n",
    "        logger.info(\"Warming up model...\")\n",
    "        \n",
    "        try:\n",
    "            # Get maximum sequence length from config\n",
    "            max_seq_len = self.config.get('max_seq_length', 512)  # Default to 512\n",
    "            \n",
    "            # Create dummy input tensor for warmup\n",
    "            dummy_input = torch.randint(\n",
    "                low=10,  # Start from 10 to avoid special token IDs (0-4)\n",
    "                high=100,  # Random IDs up to 99\n",
    "                size=(1, min(32, max_seq_len)),  # Shape: (batch_size=1, sequence_length)\n",
    "                # Use smaller sequence for warmup (max 32 tokens)\n",
    "                device=self.device  # Place on correct device (GPU/CPU)\n",
    "            )\n",
    "            \n",
    "            # Perform warmup inference\n",
    "            with torch.no_grad():  # Disable gradient computation (inference only)\n",
    "                if self.model_format == 'ONNX':\n",
    "                    # ONNX Runtime warmup\n",
    "                    input_name = self.model.get_inputs()[0].name  # Get input tensor name\n",
    "                    self.model.run(None, {input_name: dummy_input.cpu().numpy()})\n",
    "                    # .run(): Execute ONNX model\n",
    "                    # None: Don't specify output names (get all outputs)\n",
    "                    # Convert tensor to numpy for ONNX Runtime\n",
    "                else:\n",
    "                    # PyTorch/TorchScript warmup\n",
    "                    _ = self.model(dummy_input)  # Run inference, ignore output\n",
    "            \n",
    "            logger.info(\"Model warmup completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Non-critical error: Log warning but don't crash\n",
    "            logger.warning(f\"Model warmup failed (non-critical): {e}\")\n",
    "    \n",
    "    def _load_api_keys(self) -> set:\n",
    "        \"\"\"\n",
    "        Load API keys from environment or file.\n",
    "        In production, use proper secrets management like HashiCorp Vault, AWS Secrets Manager, etc.\n",
    "        \n",
    "        Returns:\n",
    "            Set of valid API keys (sets provide O(1) lookup time)\n",
    "        \"\"\"\n",
    "        # Import os module for environment variable access\n",
    "        import os\n",
    "        \n",
    "        # Get API keys from environment variable\n",
    "        api_keys_env = os.getenv('CYBERGUARD_API_KEYS', '')\n",
    "        # os.getenv(): Get environment variable value\n",
    "        # Second parameter: Default value if variable not set\n",
    "        \n",
    "        if api_keys_env:  # Check if environment variable is not empty\n",
    "            # Split comma-separated keys and convert to set\n",
    "            keys = set(api_keys_env.split(','))\n",
    "            # set(): Creates unordered collection of unique elements\n",
    "            # .split(','): Splits string at commas\n",
    "            \n",
    "            logger.info(f\"Loaded {len(keys)} API key(s) from environment\")\n",
    "        else:\n",
    "            # Fallback to single default key (for development only)\n",
    "            keys = {'cyberguard-dev-key-2024'}  # Set with single default key\n",
    "            logger.warning(\"Using default development API key. Not suitable for production.\")\n",
    "        \n",
    "        return keys  # Return set of API keys\n",
    "    \n",
    "    @lru_cache(maxsize=1000)  # Decorator: Least Recently Used cache\n",
    "    # maxsize=1000: Cache up to 1000 unique calls\n",
    "    # Automatically evicts least recently used entries when full\n",
    "    def _tokenize_cached(self, text: str, max_length: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Tokenize text with caching for repeated similar inputs.\n",
    "        LRU cache improves performance for common requests.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to tokenize\n",
    "            max_length: Maximum sequence length\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of token IDs with shape (1, max_length)\n",
    "        \"\"\"\n",
    "        # Convert to lowercase for consistency\n",
    "        # Lowercasing helps with token matching but preserves case for certain patterns\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Initialize empty token list\n",
    "        tokens = []\n",
    "        \n",
    "        # Split text into words (basic tokenization)\n",
    "        # In production, use proper tokenizer like BPE, WordPiece, or SentencePiece\n",
    "        words = text_lower.split()  # .split(): Splits on whitespace\n",
    "        \n",
    "        # Process each word up to max_length\n",
    "        for word in words[:max_length]:\n",
    "            # Check for common attack patterns\n",
    "            for pattern in ['<script>', 'javascript:', 'union', 'select', 'from', 'where']:\n",
    "                if pattern in word:  # Check if pattern exists in word\n",
    "                    # Get token ID for pattern, fallback to UNK token if not found\n",
    "                    pattern_id = self.tokenizer.get(pattern, self.tokenizer.get('[UNK]', 1))\n",
    "                    tokens.append(pattern_id)  # Add pattern token\n",
    "                    \n",
    "                    # Remove pattern from word to tokenize remaining parts\n",
    "                    word = word.replace(pattern, '')\n",
    "            \n",
    "            # Add remaining word if not empty after pattern removal\n",
    "            if word:  # Check if word is not empty string\n",
    "                # Get token ID, fallback to UNK token if not in vocabulary\n",
    "                token_id = self.tokenizer.get(word, self.tokenizer.get('[UNK]', 1))\n",
    "                tokens.append(token_id)\n",
    "        \n",
    "        # Add CLS (classification) token at beginning\n",
    "        # CLS token is commonly used in transformer models for classification tasks\n",
    "        tokens = [self.tokenizer.get('[CLS]', 2)] + tokens\n",
    "        # + tokens: Concatenate lists\n",
    "        \n",
    "        # --- SEQUENCE LENGTH MANAGEMENT ---\n",
    "        # Truncate if sequence exceeds maximum length\n",
    "        if len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length]  # Keep first max_length tokens\n",
    "        \n",
    "        # Pad if sequence is shorter than maximum length\n",
    "        elif len(tokens) < max_length:\n",
    "            # Create padding tokens (PAD token ID)\n",
    "            padding = [self.tokenizer.get('[PAD]', 0)] * (max_length - len(tokens))\n",
    "            # [value] * n: Creates list with n copies of value\n",
    "            \n",
    "            tokens = tokens + padding  # Append padding tokens\n",
    "        \n",
    "        # Convert to PyTorch tensor\n",
    "        return torch.tensor([tokens], dtype=torch.long, device=self.device)\n",
    "        # [tokens]: Create batch dimension (batch_size=1)\n",
    "        # dtype=torch.long: Integer data type for token IDs\n",
    "        # device=self.device: Place tensor on correct device (GPU/CPU)\n",
    "    \n",
    "    def _get_severity_level(self, score: float) -> str:\n",
    "        \"\"\"\n",
    "        Convert numerical severity score to categorical level.\n",
    "        \n",
    "        Args:\n",
    "            score: Severity score between 0 and 1\n",
    "            \n",
    "        Returns:\n",
    "            Severity level string (INFO, LOW, MEDIUM, HIGH, CRITICAL)\n",
    "        \"\"\"\n",
    "        # Define severity thresholds\n",
    "        if score < 0.2:\n",
    "            return 'INFO'  # Informational, not a threat\n",
    "        elif score < 0.4:\n",
    "            return 'LOW'  # Low severity threat\n",
    "        elif score < 0.6:\n",
    "            return 'MEDIUM'  # Medium severity threat\n",
    "        elif score < 0.8:\n",
    "            return 'HIGH'  # High severity threat\n",
    "        else:\n",
    "            return 'CRITICAL'  # Critical threat requiring immediate attention\n",
    "    \n",
    "    def analyze(self, text: str, detailed: bool = False, request_id: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main analysis method that processes text and returns threat analysis.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to analyze\n",
    "            detailed: Whether to include detailed probabilities\n",
    "            request_id: Optional request ID for tracking\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing analysis results\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: If model inference fails\n",
    "            ValueError: If input text is invalid\n",
    "        \"\"\"\n",
    "        # Increment request counter for statistics\n",
    "        self.request_count += 1\n",
    "        \n",
    "        # Validate input\n",
    "        if not text or not isinstance(text, str):\n",
    "            raise ValueError(\"Input text must be a non-empty string\")\n",
    "        \n",
    "        # Get maximum sequence length from configuration\n",
    "        max_seq_length = self.config.get('max_seq_length', 512)  # Default to 512\n",
    "        \n",
    "        try:\n",
    "            # --- TOKENIZATION ---\n",
    "            # Convert text to tensor using cached tokenization\n",
    "            input_tensor = self._tokenize_cached(text, max_seq_length)\n",
    "            # Shape: (1, max_seq_length) where 1 is batch size\n",
    "            \n",
    "            # --- INFERENCE BASED ON MODEL FORMAT ---\n",
    "            if self.model_format == 'ONNX':\n",
    "                # ONNX Runtime inference path\n",
    "                input_name = self.model.get_inputs()[0].name  # Get input tensor name\n",
    "                # Create input feed dictionary for ONNX Runtime\n",
    "                input_feed = {input_name: input_tensor.cpu().numpy()}\n",
    "                # .cpu(): Move tensor to CPU (ONNX expects numpy arrays)\n",
    "                # .numpy(): Convert to numpy array\n",
    "                \n",
    "                # Run inference\n",
    "                outputs = self.model.run(None, input_feed)\n",
    "                # None: Get all outputs\n",
    "                # Returns list of numpy arrays\n",
    "                \n",
    "                # Parse outputs (order depends on model export)\n",
    "                if len(outputs) >= 2:  # At least 2 outputs expected\n",
    "                    threat_logits = torch.from_numpy(outputs[0])  # Convert back to tensor\n",
    "                    severity_score = torch.from_numpy(outputs[1])\n",
    "                else:  # Only one output\n",
    "                    threat_logits = torch.from_numpy(outputs[0])\n",
    "                    severity_score = None\n",
    "                    \n",
    "            else:\n",
    "                # PyTorch/TorchScript inference path\n",
    "                with torch.no_grad():  # Disable gradient computation\n",
    "                    outputs = self.model(input_tensor)  # Run model\n",
    "                \n",
    "                # Handle different output formats from different model types\n",
    "                if isinstance(outputs, dict):\n",
    "                    # Model returns dictionary\n",
    "                    threat_logits = outputs.get('threat_logits')  # Get with .get() (returns None if not found)\n",
    "                    severity_score = outputs.get('severity_score')\n",
    "                elif isinstance(outputs, (list, tuple)):\n",
    "                    # Model returns list or tuple\n",
    "                    threat_logits = outputs[0] if len(outputs) > 0 else None\n",
    "                    severity_score = outputs[1] if len(outputs) > 1 else None\n",
    "                else:\n",
    "                    # Model returns single tensor\n",
    "                    threat_logits = outputs\n",
    "                    severity_score = None\n",
    "            \n",
    "            # --- PROCESS THREAT CLASSIFICATION OUTPUT ---\n",
    "            if threat_logits is not None:\n",
    "                # Apply softmax to convert logits to probabilities\n",
    "                probs = torch.softmax(threat_logits, dim=-1)\n",
    "                # softmax: Converts logits to probability distribution (sums to 1)\n",
    "                # dim=-1: Apply along last dimension (class dimension)\n",
    "                \n",
    "                # Get predicted class (index with highest probability)\n",
    "                threat_idx = torch.argmax(probs).item()\n",
    "                # torch.argmax(): Returns index of maximum value\n",
    "                # .item(): Convert single-element tensor to Python scalar\n",
    "                \n",
    "                # Get confidence score (probability of predicted class)\n",
    "                confidence = probs[0, threat_idx].item()\n",
    "                # probs[0, threat_idx]: Access probability for batch 0, predicted class\n",
    "                \n",
    "                # Get threat category information\n",
    "                if threat_idx < len(self.threat_categories):\n",
    "                    threat_category = self.threat_categories[threat_idx]  # Get category object\n",
    "                    category_name = threat_category.name  # Get category name\n",
    "                else:\n",
    "                    category_name = 'unknown'  # Fallback if index out of range\n",
    "                    threat_category = None\n",
    "                \n",
    "                # Check if malicious (assuming first category is benign)\n",
    "                is_malicious = threat_idx != 0\n",
    "                # Index 0 is assumed to be 'benign' category\n",
    "                \n",
    "                # Update threat detection counter\n",
    "                if is_malicious:\n",
    "                    self.threat_detection_count += 1\n",
    "                \n",
    "                # Prepare threat detection result dictionary\n",
    "                threat_result = {\n",
    "                    'category': category_name,  # Threat category name\n",
    "                    'category_id': threat_idx,  # Numerical category ID\n",
    "                    'confidence': confidence,  # Model confidence (0 to 1)\n",
    "                    'is_malicious': is_malicious  # Boolean indicating threat\n",
    "                }\n",
    "                \n",
    "                # Add detailed probabilities if requested\n",
    "                if detailed and threat_category is not None:\n",
    "                    # Create dictionary mapping category names to probabilities\n",
    "                    threat_result['probabilities'] = {\n",
    "                        cat.name: probs[0, i].item()  # Probability for each category\n",
    "                        for i, cat in enumerate(self.threat_categories)  # Loop through all categories\n",
    "                    }\n",
    "                    # Dictionary comprehension: Creates dict from loop\n",
    "                    \n",
    "            else:  # No threat logits returned\n",
    "                threat_result = {\n",
    "                    'category': 'unknown',\n",
    "                    'category_id': -1,\n",
    "                    'confidence': 0.0,\n",
    "                    'is_malicious': False\n",
    "                }\n",
    "            \n",
    "            # --- PROCESS SEVERITY SCORE ---\n",
    "            if severity_score is not None:\n",
    "                if isinstance(severity_score, torch.Tensor):\n",
    "                    severity_value = severity_score[0].item()  # Extract scalar value\n",
    "                else:\n",
    "                    severity_value = float(severity_score)  # Convert to float\n",
    "                \n",
    "                severity_result = {\n",
    "                    'score': severity_value,  # Numerical score (0 to 1)\n",
    "                    'level': self._get_severity_level(severity_value)  # Categorical level\n",
    "                }\n",
    "            else:\n",
    "                # Default severity if model doesn't provide it\n",
    "                severity_result = {\n",
    "                    'score': 0.8 if threat_result['is_malicious'] else 0.1,\n",
    "                    'level': 'HIGH' if threat_result['is_malicious'] else 'INFO'\n",
    "                }\n",
    "            \n",
    "            # --- COMPILE FINAL RESULT ---\n",
    "            result = {\n",
    "                'text_preview': text[:100] + ('...' if len(text) > 100 else ''),\n",
    "                # Show first 100 characters with ellipsis if truncated\n",
    "                \n",
    "                'text_length': len(text),  # Original text length\n",
    "                'threat_detection': threat_result,  # Threat analysis\n",
    "                'severity': severity_result,  # Severity assessment\n",
    "                'processing_time_ms': 0,  # Placeholder (would be calculated with timers)\n",
    "                'model_format': self.model_format  # Model format used\n",
    "            }\n",
    "            \n",
    "            # Log analysis completion\n",
    "            logger.info(f\"Analysis completed: threat={threat_result['category']}, \"\n",
    "                       f\"malicious={threat_result['is_malicious']}, \"\n",
    "                       f\"confidence={threat_result['confidence']:.3f}\")\n",
    "            \n",
    "            return result  # Return analysis results\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Catch any error during analysis\n",
    "            error_msg = f\"Analysis failed: {str(e)}\"\n",
    "            logger.error(error_msg, exc_info=True)  # Log with exception info\n",
    "            # exc_info=True: Include exception traceback in log\n",
    "            raise RuntimeError(error_msg)  # Re-raise as RuntimeError\n",
    "    \n",
    "    def verify_api_key(self, credentials: Optional[HTTPAuthorizationCredentials] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Verify API key from request credentials.\n",
    "        \n",
    "        Args:\n",
    "            credentials: HTTP authorization credentials\n",
    "            \n",
    "        Returns:\n",
    "            True if API key is valid, False otherwise\n",
    "        \"\"\"\n",
    "        if credentials is None:\n",
    "            logger.warning(\"No API credentials provided\")\n",
    "            return False\n",
    "        \n",
    "        # Check if credential is in valid API keys set\n",
    "        if credentials.credentials in self.api_keys:\n",
    "            return True\n",
    "        else:\n",
    "            # Log partial key for security (don't log full key)\n",
    "            logger.warning(f\"Invalid API key attempt: {credentials.credentials[:8]}...\")\n",
    "            return False\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get API usage statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with statistics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'total_requests': self.request_count,\n",
    "            'threats_detected': self.threat_detection_count,\n",
    "            'threat_detection_rate': (\n",
    "                self.threat_detection_count / self.request_count \n",
    "                if self.request_count > 0 else 0\n",
    "            ),  # Calculate rate, handle division by zero\n",
    "            'model_format': self.model_format,\n",
    "            'device': str(self.device),\n",
    "            'uptime': 'N/A'  # Placeholder - would track actual uptime\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: FASTAPI APPLICATION SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Create FastAPI application with production settings\n",
    "app = FastAPI(\n",
    "    title=\"CyberGuard Security Analysis API\",  # API title (appears in docs)\n",
    "    description=\"Enterprise-grade AI-powered web security threat detection system\",  # API description\n",
    "    version=\"2.0.0\",  # API version\n",
    "    docs_url=\"/docs\",  # URL for Swagger documentation\n",
    "    redoc_url=\"/redoc\",  # URL for ReDoc documentation\n",
    "    openapi_url=\"/openapi.json\"  # URL for OpenAPI schema\n",
    ")\n",
    "\n",
    "# Add rate limiting to app state\n",
    "app.state.limiter = limiter  # Attach limiter to app state\n",
    "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
    "# Register exception handler for rate limit exceeded errors\n",
    "\n",
    "# Add CORS (Cross-Origin Resource Sharing) middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,  # CORS middleware class\n",
    "    allow_origins=[\"*\"],  # Allow all origins (restrict in production)\n",
    "    allow_credentials=True,  # Allow cookies and authentication\n",
    "    allow_methods=[\"GET\", \"POST\"],  # Allowed HTTP methods\n",
    "    allow_headers=[\"*\"],  # Allow all headers\n",
    ")\n",
    "\n",
    "# Add trusted host middleware for security\n",
    "app.add_middleware(\n",
    "    TrustedHostMiddleware,  # Trusted host middleware\n",
    "    allowed_hosts=[\"*\"]  # Allow all hosts (restrict in production)\n",
    ")\n",
    "\n",
    "# Global API instance (will be initialized on first use)\n",
    "api_instance = None\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: DEPENDENCY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_api_instance() -> CyberGuardAPI:\n",
    "    \"\"\"\n",
    "    Dependency function to get the API instance.\n",
    "    Ensures proper initialization and provides singleton access.\n",
    "    \n",
    "    Returns:\n",
    "        Initialized CyberGuardAPI instance\n",
    "        \n",
    "    Raises:\n",
    "        HTTPException: If API fails to initialize\n",
    "    \"\"\"\n",
    "    global api_instance  # Declare we're modifying the global variable\n",
    "    \n",
    "    if api_instance is None:  # Check if not initialized\n",
    "        try:\n",
    "            # Import os here to avoid import at module level if only used in this function\n",
    "            import os\n",
    "            \n",
    "            # Get paths from environment variables with defaults\n",
    "            model_dir = Path(os.getenv('MODEL_DIR', './models'))\n",
    "            # os.getenv(): Get environment variable, default to './models'\n",
    "            \n",
    "            config_path = Path(os.getenv('CONFIG_PATH', './config.yaml'))\n",
    "            tokenizer_path = Path(os.getenv('TOKENIZER_PATH', './models/tokenizer.json'))\n",
    "            \n",
    "            # Initialize API instance\n",
    "            api_instance = CyberGuardAPI(model_dir, config_path, tokenizer_path)\n",
    "        except Exception as e:\n",
    "            # Log critical error and raise HTTP exception\n",
    "            logger.critical(f\"Failed to initialize API: {e}\")\n",
    "            raise HTTPException(\n",
    "                status_code=503,  # Service Unavailable status code\n",
    "                detail=\"Service temporarily unavailable. Failed to initialize model.\"\n",
    "            )\n",
    "    \n",
    "    return api_instance  # Return the initialized instance\n",
    "\n",
    "\n",
    "async def verify_api_key_dependency(\n",
    "    request: Request,  # FastAPI request object (automatically injected)\n",
    "    credentials: Optional[HTTPAuthorizationCredentials] = Security(security)\n",
    "    # Security(security): Use HTTP Bearer security scheme\n",
    "    # Optional: Credentials might be None if not provided\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    FastAPI dependency for API key verification.\n",
    "    This function will be called by FastAPI for endpoints that require authentication.\n",
    "    \n",
    "    Args:\n",
    "        request: FastAPI request object\n",
    "        credentials: HTTP authorization credentials\n",
    "        \n",
    "    Returns:\n",
    "        API key if valid\n",
    "        \n",
    "    Raises:\n",
    "        HTTPException: If API key is invalid or missing\n",
    "    \"\"\"\n",
    "    # Get API instance\n",
    "    api = get_api_instance()\n",
    "    \n",
    "    # Verify API key\n",
    "    if not api.verify_api_key(credentials):\n",
    "        raise HTTPException(\n",
    "            status_code=401,  # Unauthorized status code\n",
    "            detail=\"Invalid or missing API key\",\n",
    "            headers={\"WWW-Authenticate\": \"Bearer\"},  # Required for 401 responses\n",
    "        )\n",
    "    \n",
    "    # Return valid API key for use in endpoint if needed\n",
    "    return credentials.credentials\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: FASTAPI EVENT HANDLERS\n",
    "# ============================================================================\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"\n",
    "    Event handler called when FastAPI application starts.\n",
    "    Used for initialization tasks.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting CyberGuard API server...\")\n",
    "    \n",
    "    # Pre-initialize API instance during startup\n",
    "    # This helps catch initialization errors early\n",
    "    try:\n",
    "        _ = get_api_instance()  # Initialize API, ignore return value\n",
    "        logger.info(\"API initialization completed during startup\")\n",
    "    except Exception as e:\n",
    "        # Log error but don't crash - allows health checks to report status\n",
    "        logger.error(f\"Startup initialization failed: {e}\")\n",
    "        # Don't raise here to allow health endpoint to report status\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: API ENDPOINTS\n",
    "# ============================================================================\n",
    "\n",
    "@app.get(\"/health\", tags=[\"monitoring\"])  # GET endpoint, categorized under \"monitoring\"\n",
    "@limiter.limit(\"10/minute\")  # Rate limit: 10 requests per minute per IP\n",
    "async def health_check(request: Request):\n",
    "    \"\"\"\n",
    "    Health check endpoint for load balancers and monitoring.\n",
    "    Used by Kubernetes, Docker, load balancers to check service health.\n",
    "    \n",
    "    Returns:\n",
    "        Health status and basic system information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get API instance\n",
    "        api = get_api_instance()\n",
    "        \n",
    "        # Basic system checks\n",
    "        health_status = {\n",
    "            \"status\": \"healthy\",\n",
    "            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",  # ISO format with Z for UTC\n",
    "            \"model_loaded\": api.model is not None,  # Check if model loaded\n",
    "            \"model_format\": api.model_format,  # Model format (TorchScript, ONNX, etc.)\n",
    "            \"device\": str(api.device),  # Computation device\n",
    "            \"threat_categories\": len(api.threat_categories),  # Number of categories\n",
    "            \"tokenizer_size\": len(api.tokenizer)  # Vocabulary size\n",
    "        }\n",
    "        \n",
    "        return health_status  # FastAPI automatically converts dict to JSON\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Log error and return unhealthy status\n",
    "        logger.error(f\"Health check failed: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=503,  # Service Unavailable\n",
    "            detail=f\"Service unhealthy: {str(e)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "@app.post(\"/analyze\", response_model=AnalysisResponse, tags=[\"analysis\"])\n",
    "# POST endpoint, returns AnalysisResponse model, categorized under \"analysis\"\n",
    "@limiter.limit(\"100/minute\")  # Rate limit: 100 requests per minute per IP\n",
    "async def analyze_security(\n",
    "    request: Request,  # FastAPI request object\n",
    "    analysis_request: AnalysisRequest,  # Validated request body\n",
    "    api_key: str = Depends(verify_api_key_dependency)  # API key verification dependency\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze text for security threats.\n",
    "    Main endpoint for security analysis.\n",
    "    \n",
    "    Args:\n",
    "        request: FastAPI request object\n",
    "        analysis_request: Validated analysis request\n",
    "        api_key: Verified API key (from dependency)\n",
    "        \n",
    "    Returns:\n",
    "        Analysis results with standardized response format\n",
    "    \"\"\"\n",
    "    # Generate unique request ID for tracking\n",
    "    import uuid  # Import here to avoid unnecessary import at module level\n",
    "    request_id = str(uuid.uuid4())[:8]  # First 8 characters of UUID\n",
    "    # UUID: Universally Unique Identifier\n",
    "    # [:8]: Slice to get first 8 characters (sufficient for tracking)\n",
    "    \n",
    "    logger.info(f\"Analysis request {request_id}: length={len(analysis_request.text)}\")\n",
    "    \n",
    "    try:\n",
    "        # Get API instance\n",
    "        api = get_api_instance()\n",
    "        \n",
    "        # Start timing for performance measurement\n",
    "        import time  # Import here for local use\n",
    "        start_time = time.time()  # Current time in seconds since epoch\n",
    "        \n",
    "        # Perform analysis\n",
    "        analysis_result = api.analyze(\n",
    "            text=analysis_request.text,\n",
    "            detailed=analysis_request.detailed,\n",
    "            request_id=request_id\n",
    "        )\n",
    "        \n",
    "        # Calculate processing time\n",
    "        processing_time_ms = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "        analysis_result['processing_time_ms'] = processing_time_ms  # Add to result\n",
    "        \n",
    "        # Prepare response using Pydantic model\n",
    "        response = AnalysisResponse(\n",
    "            success=True,\n",
    "            analysis=analysis_result,\n",
    "            timestamp=datetime.utcnow().isoformat() + \"Z\",  # Current UTC time\n",
    "            model_version=api.config.get('model_version', '2.0.0'),  # From config\n",
    "            request_id=request_id  # Include request ID for tracking\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Analysis request {request_id} completed in {processing_time_ms:.2f}ms\")\n",
    "        \n",
    "        return response  # FastAPI automatically serializes to JSON\n",
    "        \n",
    "    except ValueError as e:\n",
    "        # Client error (invalid input)\n",
    "        logger.warning(f\"Analysis request {request_id} failed validation: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=400,  # Bad Request\n",
    "            detail=str(e)  # Include validation error message\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        # Server error during analysis\n",
    "        logger.error(f\"Analysis request {request_id} failed: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,  # Internal Server Error\n",
    "            detail=\"Analysis failed due to server error\"  # Generic error message for security\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Unexpected error\n",
    "        logger.error(f\"Unexpected error in analysis request {request_id}: {e}\", exc_info=True)\n",
    "        raise HTTPException(\n",
    "            status_code=500,  # Internal Server Error\n",
    "            detail=\"Internal server error\"  # Generic message for security\n",
    "        )\n",
    "\n",
    "\n",
    "@app.get(\"/statistics\", tags=[\"monitoring\"])\n",
    "async def get_statistics(api_key: str = Depends(verify_api_key_dependency)):\n",
    "    \"\"\"\n",
    "    Get API usage statistics.\n",
    "    Requires valid API key.\n",
    "    \n",
    "    Returns:\n",
    "        API usage statistics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        api = get_api_instance()\n",
    "        stats = api.get_statistics()  # Get statistics from API instance\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"statistics\": stats,\n",
    "            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get statistics: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=\"Failed to retrieve statistics\"\n",
    "        )\n",
    "\n",
    "\n",
    "@app.get(\"/categories\", tags=[\"information\"])\n",
    "async def get_threat_categories():\n",
    "    \"\"\"\n",
    "    Get information about available threat categories.\n",
    "    No API key required for this endpoint (public information).\n",
    "    \n",
    "    Returns:\n",
    "        List of threat categories with descriptions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        api = get_api_instance()\n",
    "        \n",
    "        # Convert ThreatCategory objects to dictionaries\n",
    "        categories = [\n",
    "            {\n",
    "                \"id\": cat.id,\n",
    "                \"name\": cat.name,\n",
    "                \"description\": cat.description,\n",
    "                \"severity_weight\": cat.severity_weight\n",
    "            }\n",
    "            for cat in api.threat_categories  # List comprehension\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"categories\": categories,\n",
    "            \"count\": len(categories)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get threat categories: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=\"Failed to retrieve threat categories\"\n",
    "        )\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 10: APPLICATION ENTRY POINT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Entry point for running the API server directly.\n",
    "    In production, use uvicorn or gunicorn instead.\n",
    "    This block only executes when script is run directly (not imported).\n",
    "    \"\"\"\n",
    "    import uvicorn  # ASGI server for running FastAPI\n",
    "    import os  # For environment variable access\n",
    "    \n",
    "    # Load environment variables from .env file if present\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  # Loads variables from .env file into os.environ\n",
    "    \n",
    "    # Configuration from environment variables with defaults\n",
    "    host = os.getenv(\"HOST\", \"0.0.0.0\")  # Host to bind to (0.0.0.0 = all interfaces)\n",
    "    port = int(os.getenv(\"PORT\", \"8000\"))  # Port to listen on\n",
    "    workers = int(os.getenv(\"WORKERS\", \"1\"))  # Number of worker processes\n",
    "    \n",
    "    # Log startup information\n",
    "    logger.info(f\"Starting CyberGuard API server on {host}:{port}\")\n",
    "    logger.info(f\"Workers: {workers}\")\n",
    "    logger.info(f\"Environment: {os.getenv('ENVIRONMENT', 'development')}\")\n",
    "    \n",
    "    # Run ASGI server\n",
    "    uvicorn.run(\n",
    "        \"cyberguard_api:app\",  # Module path to FastAPI app (this file: app variable)\n",
    "        host=host,\n",
    "        port=port,\n",
    "        workers=workers,  # Number of worker processes\n",
    "        reload=False,  # Disable auto-reload (set to True for development)\n",
    "        log_level=\"info\"  # Logging level for uvicorn\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
