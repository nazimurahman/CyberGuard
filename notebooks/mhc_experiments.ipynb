{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd5372",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Markdown - Notebook Overview\n",
    "\"\"\"\n",
    "# Manifold-Constrained Hyper-Connections (mHC) Experiments\n",
    "\n",
    "## Overview\n",
    "This notebook explores the Manifold-Constrained Hyper-Connections (mHC) architecture used in CyberGuard for stable multi-agent coordination.\n",
    "\n",
    "## Why mHC?\n",
    "Traditional multi-agent systems suffer from:\n",
    "1. **Signal Explosion**: Unbounded information flow between agents causing instability\n",
    "2. **Dominant Agent Bias**: One agent overwhelming others' contributions leading to unfair coordination\n",
    "3. **Reasoning Collapse**: Agents losing individual reasoning capabilities due to excessive mixing\n",
    "\n",
    "mHC solves these through mathematical constraints:\n",
    "- Doubly-stochastic normalization (Sinkhorn-Knopp projection) ensures equal contribution\n",
    "- Convex state mixing with bounded propagation prevents signal explosion\n",
    "- Identity-preserving mappings maintain agent individuality\n",
    "- Non-expansive updates guarantee stability\n",
    "\"\"\"\n",
    "\n",
    "# Cell 2: Setup and Imports\n",
    "import torch  # PyTorch for tensor operations and neural networks\n",
    "import torch.nn as nn  # Neural network module for building models\n",
    "import torch.nn.functional as F  # Functional operations like softmax, normalization\n",
    "import numpy as np  # Numerical computing for array operations\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "from typing import List, Tuple, Dict, Optional  # Type hints for better code documentation\n",
    "import math  # Mathematical functions and constants\n",
    "from tqdm import tqdm  # Progress bars for long-running operations\n",
    "import seaborn as sns  # Statistical data visualization (not used but imported)\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import time  # Time measurement for performance tracking\n",
    "import os  # Operating system interface for file operations\n",
    "import json  # JSON serialization for saving results\n",
    "import pickle  # Python object serialization for saving models\n",
    "\n",
    "# Set random seeds for reproducibility across runs\n",
    "torch.manual_seed(42)  # PyTorch random seed\n",
    "np.random.seed(42)  # NumPy random seed\n",
    "\n",
    "# Enable GPU acceleration if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")  # Inform user about computation device\n",
    "\n",
    "# Cell 3: mHC Core Implementation\n",
    "class ManifoldConstrainedHyperConnections:\n",
    "    \"\"\"\n",
    "    Main implementation of Manifold-Constrained Hyper-Connections (mHC).\n",
    "    This class provides mathematical guarantees for stable multi-agent coordination\n",
    "    by enforcing constraints on information flow between agents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents: int, state_dim: int, temperature: float = 1.0):\n",
    "        \"\"\"\n",
    "        Constructor initializes mHC with configuration parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        n_agents: Number of coordinating agents in the system\n",
    "        state_dim: Dimension of each agent's state vector representation\n",
    "        temperature: Controls attention distribution sharpness (higher = more uniform)\n",
    "        \"\"\"\n",
    "        # Store configuration parameters\n",
    "        self.n_agents = n_agents  # Number of agents in the coordination system\n",
    "        self.state_dim = state_dim  # Dimensionality of agent state vectors\n",
    "        self.temperature = temperature  # Attention temperature parameter\n",
    "        \n",
    "        # Sinkhorn-Knopp algorithm parameters for doubly-stochastic normalization\n",
    "        self.sinkhorn_iterations = 50  # Maximum iterations for convergence\n",
    "        self.epsilon = 1e-8  # Small constant to prevent numerical instability (division by zero)\n",
    "        \n",
    "        # Bounded propagation parameters for stability guarantees\n",
    "        self.signal_bound = 1.0  # Maximum allowed L2 norm of mixed states (β parameter)\n",
    "        self.identity_preserve_factor = 0.1  # λ parameter: weight for preserving agent identity\n",
    "        \n",
    "        # Metrics tracking for analysis and debugging\n",
    "        self.metrics = {\n",
    "            'signal_norms': [],  # Track state norms before/after bounding\n",
    "            'attention_entropy': [],  # Measure attention distribution uniformity\n",
    "            'coordination_efficiency': []  # Ratio of useful information after coordination\n",
    "        }\n",
    "        \n",
    "    def sinkhorn_knopp_projection(self, log_alpha: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Implements Sinkhorn-Knopp algorithm to convert any non-negative matrix\n",
    "        into a doubly-stochastic matrix (rows and columns sum to 1).\n",
    "        \n",
    "        Mathematical basis: Iterative row and column normalization that converges\n",
    "        to a matrix where each agent gives and receives equal total attention.\n",
    "        \n",
    "        Parameters:\n",
    "        log_alpha: Input matrix in log-space for numerical stability\n",
    "        \n",
    "        Returns:\n",
    "        Doubly-stochastic matrix after projection\n",
    "        \"\"\"\n",
    "        # Validate input dimensions\n",
    "        if log_alpha.dim() != 2:  # Must be 2D matrix\n",
    "            raise ValueError(\"log_alpha must be 2D matrix\")\n",
    "        if log_alpha.shape[0] != log_alpha.shape[1] or log_alpha.shape[0] != self.n_agents:\n",
    "            raise ValueError(f\"Expected shape [{self.n_agents}, {self.n_agents}], got {log_alpha.shape}\")\n",
    "        \n",
    "        # Perform iterative normalization\n",
    "        for iteration in range(self.sinkhorn_iterations):\n",
    "            # Row normalization: ensure each agent's outgoing influence sums to 1\n",
    "            # This prevents any single agent from dominating the coordination\n",
    "            log_alpha = log_alpha - torch.logsumexp(\n",
    "                log_alpha, \n",
    "                dim=1,  # Sum across columns (agents receiving attention)\n",
    "                keepdim=True  # Maintain dimension for broadcasting\n",
    "            )\n",
    "            \n",
    "            # Column normalization: ensure each agent receives equal total attention\n",
    "            # This prevents any agent from being ignored in coordination\n",
    "            log_alpha = log_alpha - torch.logsumexp(\n",
    "                log_alpha, \n",
    "                dim=0,  # Sum across rows (agents giving attention)\n",
    "                keepdim=True\n",
    "            )\n",
    "            \n",
    "            # Early convergence optimization: check if matrix is already doubly-stochastic\n",
    "            if iteration > 10:  # Wait for initial convergence\n",
    "                row_sums = torch.exp(log_alpha).sum(dim=1)  # Actual row sums\n",
    "                col_sums = torch.exp(log_alpha).sum(dim=0)  # Actual column sums\n",
    "                # Check if both row and column sums are approximately 1\n",
    "                row_converged = torch.allclose(row_sums, torch.ones_like(row_sums), rtol=1e-4)\n",
    "                col_converged = torch.allclose(col_sums, torch.ones_like(col_sums), rtol=1e-4)\n",
    "                if row_converged and col_converged:  # Early exit if converged\n",
    "                    break\n",
    "        \n",
    "        # Convert from log-space back to probability space\n",
    "        doubly_stochastic_matrix = torch.exp(log_alpha)\n",
    "        \n",
    "        return doubly_stochastic_matrix\n",
    "    \n",
    "    def convex_state_mixing(self, \n",
    "                           agent_states: List[torch.Tensor], \n",
    "                           attention_weights: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Core mHC algorithm: Mix agent states with mathematical constraints.\n",
    "        \n",
    "        The mixing process has four key steps:\n",
    "        1. Doubly-stochastic normalization of attention weights\n",
    "        2. Convex combination of agent states\n",
    "        3. Identity preservation to maintain agent individuality\n",
    "        4. Signal bounding to prevent information explosion\n",
    "        \n",
    "        Parameters:\n",
    "        agent_states: List of each agent's state tensor [batch_size, state_dim]\n",
    "        attention_weights: Raw attention matrix [batch_size, n_agents, n_agents]\n",
    "        \n",
    "        Returns:\n",
    "        Bounded, mixed state tensor after applying all mHC constraints\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        if not agent_states:\n",
    "            raise ValueError(\"agent_states list cannot be empty\")\n",
    "        \n",
    "        # Extract batch size from first agent's state\n",
    "        batch_size = agent_states[0].shape[0]\n",
    "        \n",
    "        # Stack all agent states into single tensor for efficient computation\n",
    "        # Result shape: [batch_size, n_agents, state_dim]\n",
    "        stacked_states = torch.stack(agent_states, dim=1)\n",
    "        \n",
    "        # Ensure attention weights have proper dimensions\n",
    "        if attention_weights.dim() == 2:  # If missing batch dimension\n",
    "            attention_weights = attention_weights.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Transpose if attention matrix has wrong orientation\n",
    "        if attention_weights.shape[1] != self.n_agents:\n",
    "            if attention_weights.shape[2] == self.n_agents:\n",
    "                attention_weights = attention_weights.transpose(1, 2)\n",
    "        \n",
    "        # Step 1: Apply Sinkhorn-Knopp projection to get doubly-stochastic attention\n",
    "        log_attention = torch.log(attention_weights + self.epsilon)  # Log-space for stability\n",
    "        \n",
    "        # Handle batch processing (Sinkhorn works on 2D matrices)\n",
    "        if batch_size == 1:\n",
    "            # Single batch: squeeze, process, then unsqueeze\n",
    "            normalized_attention = self.sinkhorn_knopp_projection(\n",
    "                log_attention.squeeze(0)\n",
    "            ).unsqueeze(0)\n",
    "        else:\n",
    "            # Multiple batches: process each separately\n",
    "            normalized_attention_list = []\n",
    "            for b in range(batch_size):\n",
    "                norm_att = self.sinkhorn_knopp_projection(log_attention[b])\n",
    "                normalized_attention_list.append(norm_att)\n",
    "            normalized_attention = torch.stack(normalized_attention_list, dim=0)\n",
    "        \n",
    "        # Store normalized attention for metrics and debugging\n",
    "        self.last_normalized_attention = normalized_attention\n",
    "        \n",
    "        # Step 2: Convex combination of agent states using normalized attention\n",
    "        # Einstein summation: bij = batch i agents j agents, bjd = batch j agents dimension\n",
    "        # Result: mixed_state[b,d] = Σ_i Σ_j normalized_attention[b,i,j] * stacked_states[b,j,d]\n",
    "        mixed_state = torch.einsum('bij,bjd->bd', normalized_attention, stacked_states)\n",
    "        \n",
    "        # Step 3: Identity preservation - blend mixed state with original identities\n",
    "        identity_states = stacked_states.mean(dim=1)  # Compute mean state across agents\n",
    "        # Convex combination: mixed_state * (1-λ) + identity_states * λ\n",
    "        # λ = identity_preserve_factor controls how much original identity is preserved\n",
    "        mixed_state = (mixed_state * (1 - self.identity_preserve_factor) + \n",
    "                      identity_states * self.identity_preserve_factor)\n",
    "        \n",
    "        # Step 4: Signal bounding - prevent state norms from exploding\n",
    "        mixed_state_norm = torch.norm(mixed_state, dim=-1, keepdim=True)  # L2 norm per batch\n",
    "        # Scaling factor: min(1, bound / norm) ensures norm ≤ bound\n",
    "        scaling = torch.minimum(\n",
    "            torch.ones_like(mixed_state_norm),  # Upper bound of 1 (no scaling if norm ≤ bound)\n",
    "            self.signal_bound / (mixed_state_norm + self.epsilon)  # Scale down if norm > bound\n",
    "        )\n",
    "        bounded_state = mixed_state * scaling  # Apply scaling\n",
    "        \n",
    "        # Track metrics for analysis\n",
    "        self._track_metrics(normalized_attention, mixed_state_norm, bounded_state)\n",
    "        \n",
    "        return bounded_state\n",
    "    \n",
    "    def _track_metrics(self, attention: torch.Tensor, \n",
    "                      pre_bound_norm: torch.Tensor,\n",
    "                      bounded_state: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Internal method to track performance metrics during coordination.\n",
    "        \n",
    "        Metrics tracked:\n",
    "        1. Signal norms before and after bounding\n",
    "        2. Attention entropy (uniformity measure)\n",
    "        3. Coordination efficiency (information preservation)\n",
    "        \"\"\"\n",
    "        # 1. Track signal norm changes\n",
    "        post_bound_norm = torch.norm(bounded_state, dim=-1).mean().item()\n",
    "        self.metrics['signal_norms'].append({\n",
    "            'pre_bound': pre_bound_norm.mean().item(),  # Norm before bounding\n",
    "            'post_bound': post_bound_norm  # Norm after bounding\n",
    "        })\n",
    "        \n",
    "        # 2. Compute attention entropy (measure of fairness)\n",
    "        attention_flat = attention.flatten()  # Flatten matrix for entropy calculation\n",
    "        entropy = -torch.sum(attention_flat * torch.log(attention_flat + self.epsilon)).item()\n",
    "        self.metrics['attention_entropy'].append(entropy)  # Higher entropy = more uniform attention\n",
    "        \n",
    "        # 3. Coordination efficiency: ratio of post-bound to pre-bound norm\n",
    "        efficiency = post_bound_norm / (pre_bound_norm.mean().item() + self.epsilon)\n",
    "        self.metrics['coordination_efficiency'].append(efficiency)  # Closer to 1 = more efficient\n",
    "    \n",
    "    def residual_coordination(self, \n",
    "                            agent_outputs: List[Dict], \n",
    "                            agent_confidences: torch.Tensor) -> Dict:\n",
    "        \"\"\"\n",
    "        Higher-level coordination that preserves individual agent reasoning.\n",
    "        \n",
    "        Instead of overwriting agent decisions, this method:\n",
    "        1. Extracts reasoning states from each agent\n",
    "        2. Applies mHC mixing for coordination\n",
    "        3. Uses coordinated state to adjust (not replace) decisions\n",
    "        4. Aggregates decisions with fairness constraints\n",
    "        \n",
    "        Parameters:\n",
    "        agent_outputs: List of agent decisions with reasoning states\n",
    "        agent_confidences: Confidence scores for each agent [batch_size, n_agents]\n",
    "        \n",
    "        Returns:\n",
    "        Dictionary with coordinated decisions and analysis information\n",
    "        \"\"\"\n",
    "        # Extract reasoning states from agent outputs\n",
    "        reasoning_states = []\n",
    "        for i, output in enumerate(agent_outputs):\n",
    "            # Get reasoning state or create zero state if not provided\n",
    "            state = output.get('reasoning_state', \n",
    "                             torch.zeros((1, self.state_dim), device=agent_confidences.device))\n",
    "            # Ensure proper dimensions\n",
    "            if state.dim() == 1:\n",
    "                state = state.unsqueeze(0)  # Add batch dimension\n",
    "            elif state.dim() == 2 and state.shape[0] != 1:\n",
    "                state = state[0:1]  # Take first element if batch exists\n",
    "            reasoning_states.append(state)\n",
    "        \n",
    "        # Create attention matrix from agent confidences\n",
    "        batch_size = agent_confidences.shape[0]\n",
    "        # Pairwise attention: conf_i * conf_j gives higher weight to confident pairs\n",
    "        attention_logits = torch.einsum('bi,bj->bij', agent_confidences, agent_confidences)\n",
    "        attention_logits = attention_logits / self.temperature  # Apply temperature scaling\n",
    "        \n",
    "        # Apply mHC state mixing to get coordinated reasoning state\n",
    "        coordinated_state = self.convex_state_mixing(reasoning_states, attention_logits)\n",
    "        \n",
    "        # Process individual agent decisions with confidence weighting\n",
    "        decisions = []\n",
    "        for i, output in enumerate(agent_outputs):\n",
    "            agent_decision = output['decision']\n",
    "            agent_weight = agent_confidences[:, i:i+1]  # Extract confidence for this agent\n",
    "            \n",
    "            # Create constrained decision weighted by confidence\n",
    "            constrained_decision = {\n",
    "                'threat_level': agent_decision['threat_level'] * agent_weight,\n",
    "                'confidence': agent_decision['confidence'] * agent_weight,\n",
    "                'evidence': agent_decision.get('evidence', []),\n",
    "                'agent_id': output.get('agent_id', f'agent_{i}')\n",
    "            }\n",
    "            decisions.append(constrained_decision)\n",
    "        \n",
    "        # Aggregate decisions using normalized confidence weights\n",
    "        threat_levels = torch.stack([d['threat_level'] for d in decisions], dim=1)\n",
    "        confidences = torch.stack([d['confidence'] for d in decisions], dim=1)\n",
    "        normalized_weights = F.softmax(agent_confidences, dim=-1)  # Normalize to probability distribution\n",
    "        \n",
    "        # Weighted sum aggregation\n",
    "        final_threat = torch.sum(threat_levels * normalized_weights.unsqueeze(-1), dim=1)\n",
    "        final_confidence = torch.sum(confidences * normalized_weights.unsqueeze(-1), dim=1)\n",
    "        \n",
    "        # Aggregate evidence from all agents with confidence weighting\n",
    "        all_evidence = []\n",
    "        for i, output in enumerate(agent_outputs):\n",
    "            evidence = output['decision'].get('evidence', [])\n",
    "            # Weight evidence by agent confidence\n",
    "            agent_weight = normalized_weights[0, i].item() if batch_size == 1 else normalized_weights[:, i].mean().item()\n",
    "            for ev in evidence:\n",
    "                ev['source_confidence'] = agent_weight  # Annotate with source confidence\n",
    "                all_evidence.append(ev)\n",
    "        \n",
    "        # Sort evidence by confidence and keep top results\n",
    "        all_evidence.sort(key=lambda x: x.get('source_confidence', 0), reverse=True)\n",
    "        top_evidence = all_evidence[:10]  # Limit to top 10 pieces for stability\n",
    "        \n",
    "        # Return comprehensive coordination results\n",
    "        return {\n",
    "            'final_decision': {\n",
    "                'threat_level': final_threat,\n",
    "                'confidence': final_confidence,\n",
    "                'evidence': top_evidence\n",
    "            },\n",
    "            'coordinated_state': coordinated_state,\n",
    "            'agent_contributions': normalized_weights.tolist(),\n",
    "            'attention_matrix': attention_logits.squeeze().tolist() if batch_size == 1 else None\n",
    "        }\n",
    "\n",
    "# Cell 4: Visualization and Analysis Functions\n",
    "def visualize_mhc_components(mhc: ManifoldConstrainedHyperConnections,\n",
    "                           agent_states: List[torch.Tensor],\n",
    "                           attention_weights: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of mHC coordination process.\n",
    "    \n",
    "    Generates 4 subplots showing:\n",
    "    1. Original agent states in 2D space\n",
    "    2. Attention matrices before/after Sinkhorn normalization\n",
    "    3. State mixing process and magnitude changes\n",
    "    4. Signal bounding effect on state norms\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))  # 2x2 grid of plots\n",
    "    \n",
    "    # Convert tensors to numpy for visualization\n",
    "    states_np = [s.detach().cpu().numpy() for s in agent_states]\n",
    "    \n",
    "    # Subplot 1: Original agent states visualization\n",
    "    ax1 = axes[0, 0]\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(states_np)))  # Distinct colors for each agent\n",
    "    \n",
    "    for i, state in enumerate(states_np):\n",
    "        # Handle batch dimension and ensure 2D coordinates\n",
    "        if state.ndim == 2:  # Has batch dimension\n",
    "            state = state[0]  # Take first batch element\n",
    "        if len(state) < 2:  # Pad if state dimension < 2\n",
    "            state_padded = np.zeros(2)\n",
    "            state_padded[:len(state)] = state[:2]\n",
    "            state = state_padded\n",
    "        \n",
    "        # Plot agent state as scatter point\n",
    "        ax1.scatter(state[0], state[1], color=colors[i], \n",
    "                   s=100, label=f'Agent {i+1}', alpha=0.7)\n",
    "        ax1.annotate(f'A{i+1}', (state[0], state[1]), \n",
    "                    xytext=(5, 5), textcoords='offset points')  # Label agents\n",
    "    \n",
    "    ax1.set_xlabel('Dimension 1')\n",
    "    ax1.set_ylabel('Dimension 2')\n",
    "    ax1.set_title('Original Agent States (First 2 Dimensions)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)  # Zero lines for reference\n",
    "    ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Subplot 2: Attention matrix visualization\n",
    "    ax2 = axes[0, 1]\n",
    "    # Get original attention matrix\n",
    "    attention_original = attention_weights.detach().cpu().numpy()\n",
    "    if attention_original.ndim == 3:\n",
    "        attention_original = attention_original[0]  # Take first batch\n",
    "    \n",
    "    # Apply Sinkhorn normalization for comparison\n",
    "    log_attention = torch.log(attention_weights + 1e-8)\n",
    "    attention_sinkhorn = mhc.sinkhorn_knopp_projection(log_attention)\n",
    "    attention_sinkhorn_np = attention_sinkhorn.detach().cpu().numpy()\n",
    "    if attention_sinkhorn_np.ndim == 3:\n",
    "        attention_sinkhorn_np = attention_sinkhorn_np[0]\n",
    "    \n",
    "    # Combine original and normalized matrices side by side\n",
    "    combined_attention = np.hstack([attention_original, attention_sinkhorn_np])\n",
    "    \n",
    "    # Create heatmap visualization\n",
    "    im = ax2.imshow(combined_attention, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)  # Add color bar\n",
    "    \n",
    "    # Add dividing line and labels\n",
    "    n = mhc.n_agents\n",
    "    ax2.axvline(x=n-0.5, color='white', linewidth=2)  # Vertical divider\n",
    "    ax2.set_xticks([n//2 - 0.5, n + n//2 - 0.5])  # Position labels\n",
    "    ax2.set_xticklabels(['Original', 'Sinkhorn'])  # Matrix type labels\n",
    "    ax2.set_yticks(range(n))\n",
    "    ax2.set_yticklabels([f'A{i+1}' for i in range(n)])  # Agent labels\n",
    "    ax2.set_title('Attention Matrices: Original vs Doubly-Stochastic')\n",
    "    ax2.set_xlabel('Matrix Type')\n",
    "    ax2.set_ylabel('Agent')\n",
    "    \n",
    "    # Subplot 3: State mixing process visualization\n",
    "    ax3 = axes[1, 0]\n",
    "    # Get mixed state from mHC\n",
    "    mixed_state = mhc.convex_state_mixing(agent_states, attention_weights)\n",
    "    mixed_np = mixed_state.detach().cpu().numpy()\n",
    "    if mixed_np.ndim == 2:\n",
    "        mixed_np = mixed_np[0]\n",
    "    \n",
    "    # Calculate L2 norms for visualization\n",
    "    x_positions = np.arange(len(states_np) + 1)\n",
    "    state_magnitudes = [np.linalg.norm(s) for s in states_np]  # Individual state norms\n",
    "    mixed_magnitude = np.linalg.norm(mixed_np)  # Mixed state norm\n",
    "    \n",
    "    # Bar plot showing individual vs mixed state magnitudes\n",
    "    ax3.bar(x_positions[:-1], state_magnitudes, alpha=0.6, \n",
    "           label='Individual States')\n",
    "    ax3.bar(x_positions[-1], mixed_magnitude, alpha=0.8, \n",
    "           color='red', label='Mixed State')\n",
    "    \n",
    "    ax3.set_xlabel('State')\n",
    "    ax3.set_ylabel('Magnitude (L2 Norm)')\n",
    "    ax3.set_title('State Mixing: Individual → Coordinated')\n",
    "    ax3.set_xticks(x_positions)\n",
    "    ax3.set_xticklabels([f'A{i+1}' for i in range(len(states_np))] + ['Mixed'])\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')  # Horizontal grid only\n",
    "    \n",
    "    # Subplot 4: Signal bounding effect visualization\n",
    "    ax4 = axes[1, 1]\n",
    "    # Calculate unbounded mixing for comparison (simple average without constraints)\n",
    "    stacked_states = torch.stack(agent_states)\n",
    "    if stacked_states.dim() == 3:\n",
    "        stacked_states = stacked_states.squeeze(1)  # Remove batch if batch_size=1\n",
    "    unbounded_mixed = stacked_states.mean(dim=0)\n",
    "    \n",
    "    # Get norms for comparison\n",
    "    if unbounded_mixed.dim() == 1:\n",
    "        unbounded_norm = torch.norm(unbounded_mixed).item()\n",
    "    else:\n",
    "        unbounded_norm = torch.norm(unbounded_mixed[0]).item()\n",
    "    \n",
    "    if mixed_state.dim() == 1:\n",
    "        bounded_norm = torch.norm(mixed_state).item()\n",
    "    else:\n",
    "        bounded_norm = torch.norm(mixed_state[0]).item()\n",
    "    \n",
    "    # Bar plot comparing unbounded vs bounded mixing\n",
    "    norms = [unbounded_norm, bounded_norm]\n",
    "    labels = ['Unbounded', 'Bounded']\n",
    "    colors_bar = ['orange', 'green']\n",
    "    \n",
    "    bars = ax4.bar(labels, norms, color=colors_bar, alpha=0.7)\n",
    "    ax4.axhline(y=mhc.signal_bound, color='red', linestyle='--', \n",
    "               label=f'Bound = {mhc.signal_bound}')  # Signal bound reference line\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, norm in zip(bars, norms):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{norm:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    ax4.set_xlabel('Mixing Type')\n",
    "    ax4.set_ylabel('State Norm')\n",
    "    ax4.set_title('Signal Bounding Effect on State Norms')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()  # Adjust spacing between subplots\n",
    "    return fig\n",
    "\n",
    "def analyze_mhc_stability(mhc: ManifoldConstrainedHyperConnections,\n",
    "                         n_iterations: int = 100,\n",
    "                         noise_level: float = 0.1):\n",
    "    \"\"\"\n",
    "    Analyze stability of mHC coordination over multiple iterations.\n",
    "    \n",
    "    Tests how mHC maintains stability under changing conditions by tracking:\n",
    "    1. State norm boundedness over time\n",
    "    2. Attention entropy consistency\n",
    "    3. Coordination efficiency stability\n",
    "    \n",
    "    Parameters:\n",
    "    n_iterations: Number of coordination steps to simulate\n",
    "    noise_level: Amount of random noise added each iteration to simulate changing inputs\n",
    "    \"\"\"\n",
    "    # Initialize random agent states\n",
    "    batch_size = 1  # Single batch for simplicity\n",
    "    state_dim = mhc.state_dim\n",
    "    \n",
    "    # Create initial agent states with some structure\n",
    "    agent_states = [\n",
    "        torch.randn(batch_size, state_dim, device=device) * 0.5 + 1.0\n",
    "        for _ in range(mhc.n_agents)\n",
    "    ]\n",
    "    \n",
    "    # Initialize random confidence scores normalized to sum to 1\n",
    "    confidences = torch.rand(batch_size, mhc.n_agents, device=device)\n",
    "    confidences = F.softmax(confidences, dim=-1)\n",
    "    \n",
    "    # History tracking for analysis\n",
    "    history = {\n",
    "        'state_norms': [],  # Track state norms over iterations\n",
    "        'attention_entropy': [],  # Track attention uniformity\n",
    "        'efficiency': [],  # Track coordination efficiency\n",
    "        'coordinated_state': []  # Store coordinated states for trajectory analysis\n",
    "    }\n",
    "    \n",
    "    # Main simulation loop\n",
    "    for iteration in range(n_iterations):\n",
    "        # Add noise to simulate changing environment\n",
    "        if iteration > 0:\n",
    "            noise = torch.randn_like(agent_states[0]) * noise_level\n",
    "            agent_states = [s + noise for s in agent_states]\n",
    "        \n",
    "        # Create attention matrix from current confidences\n",
    "        attention = torch.einsum('bi,bj->bij', confidences, confidences)\n",
    "        \n",
    "        # Apply mHC coordination\n",
    "        mixed_state = mhc.convex_state_mixing(agent_states, attention)\n",
    "        \n",
    "        # Extract metrics for analysis\n",
    "        state_norm = torch.norm(mixed_state).item()\n",
    "        history['state_norms'].append(state_norm)\n",
    "        \n",
    "        # Store mHC metrics if available\n",
    "        if mhc.metrics['attention_entropy']:\n",
    "            history['attention_entropy'].append(mhc.metrics['attention_entropy'][-1])\n",
    "        if mhc.metrics['coordination_efficiency']:\n",
    "            history['efficiency'].append(mhc.metrics['coordination_efficiency'][-1])\n",
    "        \n",
    "        # Store coordinated state for trajectory analysis\n",
    "        history['coordinated_state'].append(mixed_state.detach().cpu().numpy())\n",
    "    \n",
    "    # Create stability analysis visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # Plot 1: State norm stability over time\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(history['state_norms'], linewidth=2)\n",
    "    ax1.axhline(y=mhc.signal_bound, color='red', linestyle='--', \n",
    "               label=f'Bound ({mhc.signal_bound})')  # Signal bound reference\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('State Norm')\n",
    "    ax1.set_title('State Norm Stability Over Time')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Attention entropy evolution\n",
    "    ax2 = axes[0, 1]\n",
    "    if history['attention_entropy']:\n",
    "        ax2.plot(history['attention_entropy'], linewidth=2, color='green')\n",
    "        ax2.set_xlabel('Iteration')\n",
    "        ax2.set_ylabel('Entropy')\n",
    "        ax2.set_title('Attention Uniformity (Higher = More Fair)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Coordination efficiency over time\n",
    "    ax3 = axes[1, 0]\n",
    "    if history['efficiency']:\n",
    "        ax3.plot(history['efficiency'], linewidth=2, color='purple')\n",
    "        ax3.set_xlabel('Iteration')\n",
    "        ax3.set_ylabel('Efficiency')\n",
    "        ax3.set_title('Coordination Efficiency (Closer to 1 = Better)')\n",
    "        ax3.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)  # Ideal efficiency\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: State trajectory in 2D space\n",
    "    ax4 = axes[1, 1]\n",
    "    if history['coordinated_state']:\n",
    "        states_array = np.array(history['coordinated_state'])\n",
    "        if states_array.ndim == 3:  # Handle batch dimension\n",
    "            states_array = states_array[:, 0, :]  # Take first batch\n",
    "        \n",
    "        # Plot 2D trajectory if state dimension >= 2\n",
    "        if states_array.shape[1] >= 2:\n",
    "            # Scatter plot colored by iteration\n",
    "            ax4.scatter(states_array[:, 0], states_array[:, 1], \n",
    "                       c=range(len(states_array)), cmap='viridis', \n",
    "                       alpha=0.6, s=50)\n",
    "            ax4.plot(states_array[:, 0], states_array[:, 1], \n",
    "                    alpha=0.3, color='gray')  # Connect points with line\n",
    "            \n",
    "            # Mark start and end points\n",
    "            ax4.scatter(states_array[0, 0], states_array[0, 1], \n",
    "                       color='green', s=100, label='Start', marker='o')\n",
    "            ax4.scatter(states_array[-1, 0], states_array[-1, 1], \n",
    "                       color='red', s=100, label='End', marker='s')\n",
    "            \n",
    "            ax4.set_xlabel('Dimension 1')\n",
    "            ax4.set_ylabel('Dimension 2')\n",
    "            ax4.set_title('Coordinated State Trajectory')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Calculate quantitative stability statistics\n",
    "    stats = {}\n",
    "    if history['state_norms']:\n",
    "        norms = np.array(history['state_norms'])\n",
    "        stats['norm_mean'] = norms.mean()\n",
    "        stats['norm_std'] = norms.std()\n",
    "        stats['norm_violations'] = np.sum(norms > mhc.signal_bound * 1.1)  # Count violations (10% tolerance)\n",
    "    \n",
    "    if history['attention_entropy']:\n",
    "        entropy = np.array(history['attention_entropy'])\n",
    "        stats['entropy_mean'] = entropy.mean()\n",
    "        stats['entropy_std'] = entropy.std()\n",
    "    \n",
    "    if history['efficiency']:\n",
    "        efficiency = np.array(history['efficiency'])\n",
    "        stats['efficiency_mean'] = efficiency.mean()\n",
    "        stats['efficiency_std'] = efficiency.std()\n",
    "    \n",
    "    return fig, stats\n",
    "\n",
    "# Cell 5: Comparative Analysis Functions\n",
    "def compare_coordination_strategies(n_agents: int = 5, \n",
    "                                  state_dim: int = 64,\n",
    "                                  n_trials: int = 50):\n",
    "    \"\"\"\n",
    "    Compare mHC against naive coordination strategies.\n",
    "    \n",
    "    Evaluates four coordination approaches:\n",
    "    1. mHC: Our proposed manifold-constrained method\n",
    "    2. Simple averaging: Equal weighting of all agents\n",
    "    3. Weighted averaging: Weight by confidence scores\n",
    "    4. Max confidence: Follow most confident agent\n",
    "    \n",
    "    Metrics compared:\n",
    "    - State norm stability (lower variation = better)\n",
    "    - Computation time (lower = better)\n",
    "    - Fairness (higher entropy = more equal contributions)\n",
    "    \"\"\"\n",
    "    # Initialize mHC for comparison\n",
    "    mhc = ManifoldConstrainedHyperConnections(n_agents, state_dim)\n",
    "    \n",
    "    # Results storage for each strategy\n",
    "    results = {\n",
    "        'mhc': {'norms': [], 'times': [], 'fairness': []},\n",
    "        'simple_avg': {'norms': [], 'times': [], 'fairness': []},\n",
    "        'weighted_avg': {'norms': [], 'times': [], 'fairness': []},\n",
    "        'max_conf': {'norms': [], 'times': [], 'fairness': []}\n",
    "    }\n",
    "    \n",
    "    # Run multiple trials for statistical significance\n",
    "    for trial in tqdm(range(n_trials), desc=\"Running trials\"):\n",
    "        # Generate random agent states and confidences for this trial\n",
    "        agent_states = [\n",
    "            torch.randn(1, state_dim, device=device) * 2.0 - 1.0  # Uniform distribution in [-1, 1]\n",
    "            for _ in range(n_agents)\n",
    "        ]\n",
    "        \n",
    "        confidences = torch.rand(1, n_agents, device=device)\n",
    "        confidences = F.softmax(confidences, dim=-1)  # Normalize to probability distribution\n",
    "        \n",
    "        # Create attention matrix from confidences\n",
    "        attention = torch.einsum('bi,bj->bij', confidences, confidences)\n",
    "        \n",
    "        # Strategy 1: mHC coordination\n",
    "        start_time = time.time()\n",
    "        mhc_state = mhc.convex_state_mixing(agent_states, attention)\n",
    "        mhc_time = time.time() - start_time\n",
    "        \n",
    "        mhc_norm = torch.norm(mhc_state).item()\n",
    "        # Fairness: use attention entropy from mHC metrics\n",
    "        mhc_fairness = mhc.metrics['attention_entropy'][-1] if mhc.metrics['attention_entropy'] else 0.0\n",
    "        \n",
    "        results['mhc']['norms'].append(mhc_norm)\n",
    "        results['mhc']['times'].append(mhc_time)\n",
    "        results['mhc']['fairness'].append(mhc_fairness)\n",
    "        \n",
    "        # Strategy 2: Simple averaging (equal weights)\n",
    "        start_time = time.time()\n",
    "        simple_avg = torch.stack(agent_states).mean(dim=0)\n",
    "        simple_time = time.time() - start_time\n",
    "        \n",
    "        simple_norm = torch.norm(simple_avg).item()\n",
    "        simple_fairness = math.log(n_agents)  # Maximum entropy (perfect fairness)\n",
    "        \n",
    "        results['simple_avg']['norms'].append(simple_norm)\n",
    "        results['simple_avg']['times'].append(simple_time)\n",
    "        results['simple_avg']['fairness'].append(simple_fairness)\n",
    "        \n",
    "        # Strategy 3: Weighted averaging by confidence\n",
    "        start_time = time.time()\n",
    "        stacked_states = torch.stack(agent_states, dim=1)  # [1, n_agents, dim]\n",
    "        weights = confidences.unsqueeze(-1)  # [1, n_agents, 1]\n",
    "        weighted_avg = torch.sum(stacked_states * weights, dim=1)\n",
    "        weighted_time = time.time() - start_time\n",
    "        \n",
    "        weighted_norm = torch.norm(weighted_avg).item()\n",
    "        # Fairness: entropy of confidence distribution\n",
    "        weighted_fairness = -torch.sum(confidences[0] * torch.log(confidences[0] + 1e-8)).item()\n",
    "        \n",
    "        results['weighted_avg']['norms'].append(weighted_norm)\n",
    "        results['weighted_avg']['times'].append(weighted_time)\n",
    "        results['weighted_avg']['fairness'].append(weighted_fairness)\n",
    "        \n",
    "        # Strategy 4: Max confidence (follow most confident agent)\n",
    "        start_time = time.time()\n",
    "        max_idx = torch.argmax(confidences, dim=-1).item()\n",
    "        max_conf_state = agent_states[max_idx]\n",
    "        max_conf_time = time.time() - start_time\n",
    "        \n",
    "        max_conf_norm = torch.norm(max_conf_state).item()\n",
    "        max_conf_fairness = 0.0  # Zero fairness (only one agent contributes)\n",
    "        \n",
    "        results['max_conf']['norms'].append(max_conf_norm)\n",
    "        results['max_conf']['times'].append(max_conf_time)\n",
    "        results['max_conf']['fairness'].append(max_conf_fairness)\n",
    "    \n",
    "    # Create comparison visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    strategies = list(results.keys())\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Colorblind-friendly palette\n",
    "    \n",
    "    # Plot 1: Norm distribution comparison (box plot)\n",
    "    ax1 = axes[0, 0]\n",
    "    norm_data = [results[s]['norms'] for s in strategies]\n",
    "    \n",
    "    bp = ax1.boxplot(norm_data, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    ax1.set_xticklabels([s.replace('_', ' ').title() for s in strategies])\n",
    "    ax1.set_ylabel('State Norm')\n",
    "    ax1.set_title('State Norm Distribution (Lower Variation = More Stable)')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.axhline(y=mhc.signal_bound, color='red', linestyle='--', \n",
    "               label=f'mHC Bound ({mhc.signal_bound})')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot 2: Computation time comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    time_means = [np.mean(results[s]['times']) * 1000 for s in strategies]  # Convert to ms\n",
    "    time_stds = [np.std(results[s]['times']) * 1000 for s in strategies]\n",
    "    \n",
    "    x_pos = np.arange(len(strategies))\n",
    "    bars = ax2.bar(x_pos, time_means, yerr=time_stds, \n",
    "                  color=colors, alpha=0.7, capsize=5)\n",
    "    \n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels([s.replace('_', ' ').title() for s in strategies])\n",
    "    ax2.set_ylabel('Time (ms)')\n",
    "    ax2.set_title('Computation Time Comparison (Lower = Faster)')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean in zip(bars, time_means):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{mean:.3f}ms', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 3: Fairness comparison (violin plot)\n",
    "    ax3 = axes[1, 0]\n",
    "    fairness_data = [results[s]['fairness'] for s in strategies]\n",
    "    \n",
    "    vp = ax3.violinplot(fairness_data, showmeans=True, showmedians=True)\n",
    "    \n",
    "    # Customize violin plot colors\n",
    "    for pc, color in zip(vp['bodies'], colors):\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    vp['cmeans'].set_color('black')  # Mean indicator color\n",
    "    vp['cmedians'].set_color('red')  # Median indicator color\n",
    "    \n",
    "    ax3.set_xticks(range(1, len(strategies) + 1))\n",
    "    ax3.set_xticklabels([s.replace('_', ' ').title() for s in strategies])\n",
    "    ax3.set_ylabel('Fairness (Higher = More Equal)')\n",
    "    ax3.set_title('Agent Contribution Fairness Comparison')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 4: Norm vs Fairness trade-off scatter plot\n",
    "    ax4 = axes[1, 1]\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        norms = results[strategy]['norms']\n",
    "        fairness = results[strategy]['fairness']\n",
    "        ax4.scatter(norms, fairness, color=colors[i], alpha=0.6,\n",
    "                   label=strategy.replace('_', ' ').title(), s=50)\n",
    "    \n",
    "    ax4.set_xlabel('State Norm')\n",
    "    ax4.set_ylabel('Fairness')\n",
    "    ax4.set_title('Norm vs Fairness Trade-off Analysis')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add ideal region markers\n",
    "    ideal_norm = mhc.signal_bound\n",
    "    ideal_fairness = math.log(n_agents)\n",
    "    ax4.axvline(x=ideal_norm, color='green', linestyle='--', alpha=0.5, label='Ideal Norm')\n",
    "    ax4.axhline(y=ideal_fairness, color='blue', linestyle='--', alpha=0.5, label='Ideal Fairness')\n",
    "    ax4.scatter([ideal_norm], [ideal_fairness], color='black', s=100, \n",
    "               marker='*', label='Ideal Point')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Calculate summary statistics for quantitative comparison\n",
    "    summary_stats = {}\n",
    "    for strategy in strategies:\n",
    "        summary_stats[strategy] = {\n",
    "            'norm_mean': np.mean(results[strategy]['norms']),\n",
    "            'norm_std': np.std(results[strategy]['norms']),\n",
    "            'norm_bound_violation': np.mean(\n",
    "                np.array(results[strategy]['norms']) > mhc.signal_bound * 1.1\n",
    "            ),\n",
    "            'time_mean_ms': np.mean(results[strategy]['times']) * 1000,\n",
    "            'fairness_mean': np.mean(results[strategy]['fairness']),\n",
    "            'fairness_std': np.std(results[strategy]['fairness'])\n",
    "        }\n",
    "    \n",
    "    return fig, summary_stats\n",
    "\n",
    "# Cell 6: Security-Specific Experiments\n",
    "def security_threat_coordination_experiment():\n",
    "    \"\"\"\n",
    "    Simulate real-world security threat coordination scenarios.\n",
    "    \n",
    "    Tests mHC with specialized security agents having:\n",
    "    - Different expertise areas (XSS, SQLi, CSRF, etc.)\n",
    "    - Varying confidence levels based on threat type\n",
    "    - Potential conflicting threat assessments\n",
    "    \n",
    "    Scenarios include:\n",
    "    1. Specialized attacks (XSS, SQLi)\n",
    "    2. False positives\n",
    "    3. Mixed threats\n",
    "    4. Conflicting assessments\n",
    "    \"\"\"\n",
    "    # Define security agent types with their expertise\n",
    "    agent_types = [\n",
    "        {'name': 'XSS_Detector', 'expertise': 'xss', 'base_confidence': 0.9},\n",
    "        {'name': 'SQLi_Detector', 'expertise': 'sqli', 'base_confidence': 0.8},\n",
    "        {'name': 'CSRF_Detector', 'expertise': 'csrf', 'base_confidence': 0.7},\n",
    "        {'name': 'Behavior_Analyzer', 'expertise': 'behavior', 'base_confidence': 0.6},\n",
    "        {'name': 'Payload_Scanner', 'expertise': 'malware', 'base_confidence': 0.85}\n",
    "    ]\n",
    "    \n",
    "    n_agents = len(agent_types)\n",
    "    state_dim = 128  # Reasonable state dimension for security features\n",
    "    \n",
    "    # Initialize mHC for coordination\n",
    "    mhc = ManifoldConstrainedHyperConnections(n_agents, state_dim)\n",
    "    \n",
    "    # Define test scenarios with different threat characteristics\n",
    "    scenarios = [\n",
    "        {\n",
    "            'name': 'XSS Attack',\n",
    "            'threat_type': 'xss',\n",
    "            'agent_detections': [0.95, 0.3, 0.2, 0.4, 0.1],  # XSS expert highly confident\n",
    "            'threat_level': 0.9\n",
    "        },\n",
    "        {\n",
    "            'name': 'SQL Injection',\n",
    "            'threat_type': 'sqli',\n",
    "            'agent_detections': [0.2, 0.92, 0.1, 0.3, 0.05],  # SQLi expert highly confident\n",
    "            'threat_level': 0.85\n",
    "        },\n",
    "        {\n",
    "            'name': 'False Positive',\n",
    "            'threat_type': 'benign',\n",
    "            'agent_detections': [0.1, 0.15, 0.08, 0.05, 0.12],  # All agents uncertain\n",
    "            'threat_level': 0.1\n",
    "        },\n",
    "        {\n",
    "            'name': 'Mixed Threat',\n",
    "            'threat_type': 'mixed',\n",
    "            'agent_detections': [0.7, 0.8, 0.6, 0.9, 0.75],  # All agents somewhat confident\n",
    "            'threat_level': 0.75\n",
    "        },\n",
    "        {\n",
    "            'name': 'Conflicting Assessment',\n",
    "            'threat_type': 'conflict',\n",
    "            'agent_detections': [0.9, 0.1, 0.85, 0.2, 0.15],  # Strong disagreement between agents\n",
    "            'threat_level': 0.5\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []  # Store results for each scenario\n",
    "    \n",
    "    # Run each scenario\n",
    "    for scenario in scenarios:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Scenario: {scenario['name']}\")\n",
    "        print(f\"Threat Type: {scenario['threat_type']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create agent states based on expertise and detection confidence\n",
    "        agent_states = []\n",
    "        agent_outputs = []\n",
    "        \n",
    "        for i, agent in enumerate(agent_types):\n",
    "            # Create base state with expertise encoding\n",
    "            base_state = torch.zeros(1, state_dim, device=device)\n",
    "            \n",
    "            # Encode expertise in specific dimensions (each agent gets 10 dimensions)\n",
    "            expertise_idx = i * 10\n",
    "            base_state[0, expertise_idx:expertise_idx+10] = 1.0\n",
    "            \n",
    "            # Add noise based on detection confidence (higher confidence = less noise)\n",
    "            detection_conf = scenario['agent_detections'][i]\n",
    "            noise = torch.randn_like(base_state) * (1 - detection_conf) * 0.5\n",
    "            agent_state = base_state + noise\n",
    "            \n",
    "            # Scale state by detection confidence\n",
    "            agent_state = agent_state * detection_conf\n",
    "            agent_states.append(agent_state)\n",
    "            \n",
    "            # Create agent output dictionary\n",
    "            agent_outputs.append({\n",
    "                'agent_id': agent['name'],\n",
    "                'expertise': agent['expertise'],\n",
    "                'decision': {\n",
    "                    'threat_level': torch.tensor([[detection_conf]], device=device),\n",
    "                    'confidence': torch.tensor([[detection_conf * agent['base_confidence']]], \n",
    "                                              device=device),\n",
    "                    'evidence': [\n",
    "                        f\"{agent['name']} detected {scenario['threat_type']} with confidence {detection_conf:.2f}\"\n",
    "                    ]\n",
    "                },\n",
    "                'reasoning_state': agent_state\n",
    "            })\n",
    "        \n",
    "        # Create confidence tensor and normalize\n",
    "        confidences = torch.tensor([scenario['agent_detections']], device=device)\n",
    "        confidences = F.softmax(confidences, dim=-1)\n",
    "        \n",
    "        # Perform mHC coordination\n",
    "        coordinated_result = mhc.residual_coordination(agent_outputs, confidences)\n",
    "        \n",
    "        # Extract and analyze results\n",
    "        final_threat = coordinated_result['final_decision']['threat_level'].item()\n",
    "        final_confidence = coordinated_result['final_decision']['confidence'].item()\n",
    "        agent_contributions = coordinated_result['agent_contributions'][0]\n",
    "        \n",
    "        # Calculate expert alignment for specialized threats\n",
    "        if scenario['threat_type'] in ['xss', 'sqli']:\n",
    "            expert_idx = 0 if scenario['threat_type'] == 'xss' else 1\n",
    "            expert_weight = agent_contributions[expert_idx]\n",
    "            alignment = expert_weight / max(agent_contributions)\n",
    "        else:\n",
    "            alignment = 1.0  # Not applicable for non-specialized threats\n",
    "        \n",
    "        # Calculate accuracy compared to ground truth\n",
    "        accuracy = 1.0 - abs(final_threat - scenario['threat_level'])\n",
    "        \n",
    "        # Store scenario results\n",
    "        results.append({\n",
    "            'scenario': scenario['name'],\n",
    "            'threat_type': scenario['threat_type'],\n",
    "            'ground_truth': scenario['threat_level'],\n",
    "            'final_threat': final_threat,\n",
    "            'final_confidence': final_confidence,\n",
    "            'accuracy': accuracy,\n",
    "            'expert_alignment': alignment,\n",
    "            'agent_contributions': agent_contributions\n",
    "        })\n",
    "        \n",
    "        # Print detailed results for this scenario\n",
    "        print(f\"Ground Truth Threat Level: {scenario['threat_level']:.2f}\")\n",
    "        print(f\"mHC Coordinated Threat: {final_threat:.2f}\")\n",
    "        print(f\"mHC Confidence: {final_confidence:.2f}\")\n",
    "        print(f\"Decision Accuracy: {accuracy:.2%}\")\n",
    "        \n",
    "        if scenario['threat_type'] in ['xss', 'sqli']:\n",
    "            print(f\"Expert Alignment: {alignment:.2%}\")\n",
    "        \n",
    "        print(\"\\nAgent Contributions:\")\n",
    "        for i, agent in enumerate(agent_types):\n",
    "            print(f\"  {agent['name']}: {agent_contributions[i]:.3f}\")\n",
    "    \n",
    "    # Convert results to DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Threat level comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    x_pos = np.arange(len(results_df))\n",
    "    width = 0.35  # Bar width\n",
    "    \n",
    "    bars1 = ax1.bar(x_pos - width/2, results_df['ground_truth'], \n",
    "                   width, label='Ground Truth', alpha=0.7)\n",
    "    bars2 = ax1.bar(x_pos + width/2, results_df['final_threat'], \n",
    "                   width, label='mHC Coordinated', alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Scenario')\n",
    "    ax1.set_ylabel('Threat Level')\n",
    "    ax1.set_title('Threat Level: Ground Truth vs mHC Coordination')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(results_df['scenario'], rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add error indicators between bars\n",
    "    for i, (gt, mhc_val) in enumerate(zip(results_df['ground_truth'], results_df['final_threat'])):\n",
    "        diff = abs(gt - mhc_val)\n",
    "        ax1.plot([i - width/2, i + width/2], [gt, mhc_val], \n",
    "                'k-', alpha=0.5, linewidth=1)  # Connecting line\n",
    "        ax1.text(i, max(gt, mhc_val) + 0.05, f'{diff:.3f}', \n",
    "                ha='center', va='bottom', fontsize=8)  # Difference value\n",
    "    \n",
    "    # Plot 2: Accuracy by scenario\n",
    "    ax2 = axes[0, 1]\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(results_df)))\n",
    "    \n",
    "    bars = ax2.bar(range(len(results_df)), results_df['accuracy'], color=colors)\n",
    "    ax2.set_xlabel('Scenario')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Decision Accuracy by Scenario')\n",
    "    ax2.set_xticks(range(len(results_df)))\n",
    "    ax2.set_xticklabels(results_df['scenario'], rotation=45, ha='right')\n",
    "    ax2.set_ylim([0, 1.1])\n",
    "    ax2.axhline(y=1.0, color='green', linestyle='--', alpha=0.5, label='Perfect Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for bar, acc in zip(bars, results_df['accuracy']):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{acc:.2%}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 3: Agent contribution heatmap\n",
    "    ax3 = axes[1, 0]\n",
    "    contrib_matrix = np.array([r['agent_contributions'] for r in results])\n",
    "    \n",
    "    im = ax3.imshow(contrib_matrix.T, cmap='YlOrRd', aspect='auto', \n",
    "                   interpolation='nearest')\n",
    "    plt.colorbar(im, ax=ax3, label='Contribution Weight')\n",
    "    \n",
    "    ax3.set_xlabel('Scenario')\n",
    "    ax3.set_ylabel('Agent')\n",
    "    ax3.set_title('Agent Contribution Patterns Across Scenarios')\n",
    "    ax3.set_xticks(range(len(results_df)))\n",
    "    ax3.set_xticklabels(results_df['scenario'], rotation=45, ha='right')\n",
    "    ax3.set_yticks(range(len(agent_types)))\n",
    "    ax3.set_yticklabels([a['name'] for a in agent_types])\n",
    "    \n",
    "    # Add contribution values to heatmap\n",
    "    for i in range(contrib_matrix.shape[0]):\n",
    "        for j in range(contrib_matrix.shape[1]):\n",
    "            ax3.text(i, j, f'{contrib_matrix[i, j]:.2f}', \n",
    "                    ha='center', va='center', \n",
    "                    color='black' if contrib_matrix[i, j] > 0.3 else 'white',  # Dynamic text color\n",
    "                    fontsize=8)\n",
    "    \n",
    "    # Plot 4: Expert alignment analysis\n",
    "    ax4 = axes[1, 1]\n",
    "    # Filter scenarios with specialized threats\n",
    "    expert_scenarios = results_df[results_df['threat_type'].isin(['xss', 'sqli'])]\n",
    "    \n",
    "    if not expert_scenarios.empty:\n",
    "        x_pos_exp = np.arange(len(expert_scenarios))\n",
    "        \n",
    "        # Extract expert weights and maximum weights\n",
    "        expert_weights = []\n",
    "        max_weights = []\n",
    "        \n",
    "        for _, row in expert_scenarios.iterrows():\n",
    "            scenario_idx = results_df[results_df['scenario'] == row['scenario']].index[0]\n",
    "            contributions = results[scenario_idx]['agent_contributions']\n",
    "            \n",
    "            # Identify expert based on threat type\n",
    "            if row['threat_type'] == 'xss':\n",
    "                expert_idx = 0  # XSS detector\n",
    "            else:  # sqli\n",
    "                expert_idx = 1  # SQLi detector\n",
    "            \n",
    "            expert_weights.append(contributions[expert_idx])\n",
    "            max_weights.append(max(contributions))\n",
    "        \n",
    "        # Plot expert weight vs max weight\n",
    "        ax4.bar(x_pos_exp - 0.2, expert_weights, 0.4, \n",
    "               label='Expert Weight', alpha=0.7)\n",
    "        ax4.bar(x_pos_exp + 0.2, max_weights, 0.4, \n",
    "               label='Max Weight', alpha=0.7)\n",
    "        \n",
    "        ax4.set_xlabel('Scenario')\n",
    "        ax4.set_ylabel('Contribution Weight')\n",
    "        ax4.set_title('Expert vs Max Contribution (Specialized Threats)')\n",
    "        ax4.set_xticks(x_pos_exp)\n",
    "        ax4.set_xticklabels(expert_scenarios['scenario'])\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add ratio indicators\n",
    "        for i, (exp, max_w) in enumerate(zip(expert_weights, max_weights)):\n",
    "            ratio = exp / max_w if max_w > 0 else 0\n",
    "            ax4.text(i, max(exp, max_w) + 0.05, f'{ratio:.2f}', \n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No specialized threat scenarios\\nin this experiment',\n",
    "                ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('Expert Alignment Analysis')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Calculate overall experiment statistics\n",
    "    overall_stats = {\n",
    "        'mean_accuracy': results_df['accuracy'].mean(),\n",
    "        'std_accuracy': results_df['accuracy'].std(),\n",
    "        'mean_threat_error': (results_df['ground_truth'] - results_df['final_threat']).abs().mean(),\n",
    "        'scenarios_with_high_accuracy': (results_df['accuracy'] > 0.9).sum(),\n",
    "        'total_scenarios': len(results_df)\n",
    "    }\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"OVERALL EXPERIMENT STATISTICS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Mean Accuracy: {overall_stats['mean_accuracy']:.2%}\")\n",
    "    print(f\"Accuracy Std Dev: {overall_stats['std_accuracy']:.3f}\")\n",
    "    print(f\"Mean Threat Level Error: {overall_stats['mean_threat_error']:.3f}\")\n",
    "    print(f\"Scenarios with >90% Accuracy: {overall_stats['scenarios_with_high_accuracy']}/{overall_stats['total_scenarios']}\")\n",
    "    \n",
    "    return fig, results_df, overall_stats\n",
    "\n",
    "# Cell 7: Parameter Tuning Experiment\n",
    "def parameter_tuning_experiment():\n",
    "    \"\"\"\n",
    "    Systematic exploration of mHC parameter space.\n",
    "    \n",
    "    Tests four key parameters:\n",
    "    1. Identity preservation factor (λ): Controls individuality vs mixing\n",
    "    2. Signal bound (β): Maximum allowed state norm\n",
    "    3. Temperature (τ): Attention distribution sharpness\n",
    "    4. Sinkhorn iterations: Balance of accuracy vs computation time\n",
    "    \n",
    "    Uses one-at-a-time experimental design to understand each parameter's effect.\n",
    "    \"\"\"\n",
    "    # Define parameter ranges to test\n",
    "    param_ranges = {\n",
    "        'identity_factor': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],  # λ values\n",
    "        'signal_bound': [0.5, 0.8, 1.0, 1.2, 1.5, 2.0],  # β values\n",
    "        'temperature': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0],  # τ values\n",
    "        'sinkhorn_iterations': [10, 20, 50, 100, 200]  # Iteration counts\n",
    "    }\n",
    "    \n",
    "    n_agents = 5\n",
    "    state_dim = 64\n",
    "    n_trials = 20  # Trials per parameter value for statistical significance\n",
    "    \n",
    "    results = []  # Store results for all parameter combinations\n",
    "    \n",
    "    # Parameter 1: Identity preservation factor experiment\n",
    "    print(\"Running parameter tuning experiments...\")\n",
    "    print(\"\\n1. Testing identity preservation factor...\")\n",
    "    for identity_factor in tqdm(param_ranges['identity_factor']):\n",
    "        trial_results = []\n",
    "        for trial in range(n_trials):\n",
    "            # Create test scenario\n",
    "            agent_states = [\n",
    "                torch.randn(1, state_dim, device=device) * 1.5\n",
    "                for _ in range(n_agents)\n",
    "            ]\n",
    "            confidences = torch.rand(1, n_agents, device=device)\n",
    "            confidences = F.softmax(confidences, dim=-1)\n",
    "            \n",
    "            # Create mHC with current parameter\n",
    "            mhc = ManifoldConstrainedHyperConnections(n_agents, state_dim)\n",
    "            mhc.identity_preserve_factor = identity_factor\n",
    "            \n",
    "            # Test coordination\n",
    "            attention = torch.einsum('bi,bj->bij', confidences, confidences)\n",
    "            mixed_state = mhc.convex_state_mixing(agent_states, attention)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mixed_norm = torch.norm(mixed_state).item()\n",
    "            \n",
    "            # Individuality preservation: cosine similarity with original states\n",
    "            individual_states = torch.stack(agent_states).squeeze(1)\n",
    "            similarities = F.cosine_similarity(mixed_state, individual_states, dim=-1)\n",
    "            individuality = similarities.mean().item()\n",
    "            \n",
    "            # Stability: whether norm respects bound\n",
    "            stability = 1.0 if mixed_norm <= mhc.signal_bound else 0.0\n",
    "            \n",
    "            trial_results.append({\n",
    "                'identity_factor': identity_factor,\n",
    "                'mixed_norm': mixed_norm,\n",
    "                'individuality': individuality,\n",
    "                'stability': stability\n",
    "            })\n",
    "        \n",
    "        # Aggregate trial results\n",
    "        avg_results = {\n",
    "            'parameter': 'identity_factor',\n",
    "            'value': identity_factor,\n",
    "            'avg_norm': np.mean([r['mixed_norm'] for r in trial_results]),\n",
    "            'avg_individuality': np.mean([r['individuality'] for r in trial_results]),\n",
    "            'stability_rate': np.mean([r['stability'] for r in trial_results])\n",
    "        }\n",
    "        results.append(avg_results)\n",
    "    \n",
    "    # Parameter 2: Signal bound experiment\n",
    "    print(\"\\n2. Testing signal bound...\")\n",
    "    for signal_bound in tqdm(param_ranges['signal_bound']):\n",
    "        trial_results = []\n",
    "        for trial in range(n_trials):\n",
    "            agent_states = [\n",
    "                torch.randn(1, state_dim, device=device) * 2.0\n",
    "                for _ in range(n_agents)\n",
    "            ]\n",
    "            confidences = torch.rand(1, n_agents, device=device)\n",
    "            confidences = F.softmax(confidences, dim=-1)\n",
    "            \n",
    "            mhc = ManifoldConstrainedHyperConnections(n_agents, state_dim)\n",
    "            mhc.signal_bound = signal_bound\n",
    "            \n",
    "            attention = torch.einsum('bi,bj->bij', confidences, confidences)\n",
    "            mixed_state = mhc.convex_state_mixing(agent_states, attention)\n",
    "            \n",
    "            mixed_norm = torch.norm(mixed_state).item()\n",
    "            \n",
    "            # Efficiency: how close to bound (optimal is near but not exceeding bound)\n",
    "            efficiency = mixed_norm / signal_bound if signal_bound > 0 else 0\n",
    "            utilization = min(1.0, mixed_norm / signal_bound) if signal_bound > 0 else 0\n",
    "            \n",
    "            trial_results.append({\n",
    "                'signal_bound': signal_bound,\n",
    "                'mixed_norm': mixed_norm,\n",
    "                'efficiency': efficiency,\n",
    "                'utilization': utilization\n",
    "            })\n",
    "        \n",
    "        avg_results = {\n",
    "            'parameter': 'signal_bound',\n",
    "            'value': signal_bound,\n",
    "            'avg_norm': np.mean([r['mixed_norm'] for r in trial_results]),\n",
    "            'avg_efficiency': np.mean([r['efficiency'] for r in trial_results]),\n",
    "            'avg_utilization': np.mean([r['utilization'] for r in trial_results])\n",
    "        }\n",
    "        results.append(avg_results)\n",
    "    \n",
    "    # Parameter 3: Temperature experiment\n",
    "    print(\"\\n3. Testing temperature...\")\n",
    "    for temperature in tqdm(param_ranges['temperature']):\n",
    "        trial_results = []\n",
    "        for trial in range(n_trials):\n",
    "            agent_states = [\n",
    "                torch.randn(1, state_dim, device=device)\n",
    "                for _ in range(n_agents)\n",
    "            ]\n",
    "            confidences = torch.rand(1, n_agents, device=device)\n",
    "            confidences = F.softmax(confidences, dim=-1)\n",
    "            \n",
    "            mhc = ManifoldConstrainedHyperConnections(n_agents, state_dim)\n",
    "            mhc.temperature = temperature\n",
    "            \n",
    "            attention = torch.einsum('bi,bj->bij', confidences, confidences)\n",
    "            mixed_state = mhc.convex_state_mixing(agent_states, attention)\n",
    "            \n",
    "            # Attention entropy (uniformity)\n",
    "            attention_entropy = mhc.metrics['attention_entropy'][-1] if mhc.metrics['attention_entropy'] else 0.0\n",
    "            # State variance (decision sharpness)\n",
    "            mixed_var = mixed_state.var().item()\n",
    "            \n",
    "            trial_results.append({\n",
    "                'temperature': temperature,\n",
    "                'attention_entropy': attention_entropy,\n",
    "                'mixed_variance': mixed_var\n",
    "            })\n",
    "        \n",
    "        avg_results = {\n",
    "            'parameter': 'temperature',\n",
    "            'value': temperature,\n",
    "            'avg_entropy': np.mean([r['attention_entropy'] for r in trial_results]),\n",
    "            'avg_variance': np.mean([r['mixed_variance'] for r in trial_results])\n",
    "        }\n",
    "        results.append(avg_results)\n",
    "    \n",
    "    # Parameter 4: Sinkhorn iterations experiment\n",
    "    print(\"\\n4. Testing Sinkhorn iterations...\")\n",
    "    for sinkhorn_iter in tqdm(param_ranges['sinkhorn_iterations']):\n",
    "        trial_results = []\n",
    "        for trial in range(n_trials):\n",
    "            agent_states = [\n",
    "                torch.randn(1, state_dim, device=device)\n",
    "                for _ in range(n_agents)\n",
    "            ]\n",
    "            \n",
    "            # Create non-doubly-stochastic attention matrix\n",
    "            attention = torch.rand(1, n_agents, n_agents, device=device)\n",
    "            attention = attention / attention.sum(dim=-1, keepdim=True)  # Row-stochastic only\n",
    "            \n",
    "            mhc = ManifoldConstrainedHyperConnections(n_agents, state_dim)\n",
    "            mhc.sinkhorn_iterations = sinkhorn_iter\n",
    "            \n",
    "            # Measure computation time\n",
    "            start_time = time.time()\n",
    "            mixed_state = mhc.convex_state_mixing(agent_states, attention)\n",
    "            computation_time = time.time() - start_time\n",
    "            \n",
    "            # Check convergence error\n",
    "            convergence_error = 0.0\n",
    "            if hasattr(mhc, 'last_normalized_attention'):\n",
    "                norm_att = mhc.last_normalized_attention\n",
    "                row_sums = norm_att.sum(dim=-1)\n",
    "                col_sums = norm_att.sum(dim=-2)\n",
    "                row_error = torch.abs(row_sums - 1.0).mean().item()\n",
    "                col_error = torch.abs(col_sums - 1.0).mean().item()\n",
    "                convergence_error = (row_error + col_error) / 2\n",
    "            \n",
    "            trial_results.append({\n",
    "                'sinkhorn_iterations': sinkhorn_iter,\n",
    "                'computation_time': computation_time,\n",
    "                'convergence_error': convergence_error\n",
    "            })\n",
    "        \n",
    "        avg_results = {\n",
    "            'parameter': 'sinkhorn_iterations',\n",
    "            'value': sinkhorn_iter,\n",
    "            'avg_time': np.mean([r['computation_time'] for r in trial_results]),\n",
    "            'avg_error': np.mean([r['convergence_error'] for r in trial_results])\n",
    "        }\n",
    "        results.append(avg_results)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create parameter tuning visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Identity factor analysis\n",
    "    ax1 = axes[0, 0]\n",
    "    idf_data = results_df[results_df['parameter'] == 'identity_factor']\n",
    "    ax1.plot(idf_data['value'], idf_data['avg_individuality'], \n",
    "            'o-', linewidth=2, label='Individuality', markersize=8)\n",
    "    ax1.plot(idf_data['value'], idf_data['stability_rate'], \n",
    "            's-', linewidth=2, label='Stability Rate', markersize=8)\n",
    "    ax1.set_xlabel('Identity Preservation Factor (λ)')\n",
    "    ax1.set_ylabel('Metric Value')\n",
    "    ax1.set_title('Identity Factor: Individuality vs Stability Trade-off')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Find optimal identity factor (balance point)\n",
    "    combined_score = (np.array(idf_data['avg_individuality']) + \n",
    "                     np.array(idf_data['stability_rate'])) / 2\n",
    "    optimal_idx = np.argmax(combined_score)\n",
    "    optimal_value = idf_data.iloc[optimal_idx]['value']\n",
    "    ax1.axvline(x=optimal_value, color='red', linestyle='--', alpha=0.7,\n",
    "               label=f'Optimal λ = {optimal_value:.2f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot 2: Signal bound analysis\n",
    "    ax2 = axes[0, 1]\n",
    "    bound_data = results_df[results_df['parameter'] == 'signal_bound']\n",
    "    ax2_norm = ax2  # Left y-axis for norm\n",
    "    ax2_eff = ax2.twinx()  # Right y-axis for efficiency\n",
    "    \n",
    "    line1 = ax2_norm.plot(bound_data['value'], bound_data['avg_norm'], \n",
    "                         'bo-', linewidth=2, label='Average Norm', markersize=8)\n",
    "    ax2_norm.set_xlabel('Signal Bound (β)')\n",
    "    ax2_norm.set_ylabel('Average State Norm', color='blue')\n",
    "    ax2_norm.tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    line2 = ax2_eff.plot(bound_data['value'], bound_data['avg_efficiency'], \n",
    "                        'rs-', linewidth=2, label='Efficiency', markersize=8)\n",
    "    ax2_eff.set_ylabel('Efficiency (Norm/Bound)', color='red')\n",
    "    ax2_eff.tick_params(axis='y', labelcolor='red')\n",
    "    \n",
    "    # Add utilization as shaded area\n",
    "    ax2_norm.fill_between(bound_data['value'], 0, bound_data['avg_utilization'],\n",
    "                         alpha=0.2, color='green', label='Bound Utilization')\n",
    "    \n",
    "    ax2_norm.set_title('Signal Bound: Norm vs Efficiency Trade-off')\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax2_norm.legend(lines, labels, loc='upper left')\n",
    "    ax2_norm.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Temperature analysis\n",
    "    ax3 = axes[1, 0]\n",
    "    temp_data = results_df[results_df['parameter'] == 'temperature']\n",
    "    ax3_ent = ax3  # Left y-axis for entropy\n",
    "    ax3_var = ax3.twinx()  # Right y-axis for variance\n",
    "    \n",
    "    line1 = ax3_ent.plot(temp_data['value'], temp_data['avg_entropy'], \n",
    "                        'go-', linewidth=2, label='Attention Entropy', markersize=8)\n",
    "    ax3_ent.set_xlabel('Temperature (τ)')\n",
    "    ax3_ent.set_ylabel('Attention Entropy', color='green')\n",
    "    ax3_ent.tick_params(axis='y', labelcolor='green')\n",
    "    \n",
    "    line2 = ax3_var.plot(temp_data['value'], temp_data['avg_variance'], \n",
    "                        'md-', linewidth=2, label='State Variance', markersize=8)\n",
    "    ax3_var.set_ylabel('State Variance', color='magenta')\n",
    "    ax3_var.tick_params(axis='y', labelcolor='magenta')\n",
    "    \n",
    "    ax3_ent.set_title('Temperature: Entropy vs Variance Trade-off')\n",
    "    \n",
    "    # Find temperature transition point\n",
    "    entropy_diff = np.diff(temp_data['avg_entropy'])\n",
    "    variance_diff = np.diff(temp_data['avg_variance'])\n",
    "    entropy_change = np.abs(entropy_diff)\n",
    "    variance_change = np.abs(variance_diff)\n",
    "    combined_change = entropy_change + variance_change\n",
    "    if len(combined_change) > 0:\n",
    "        max_change_idx = np.argmax(combined_change)\n",
    "        if max_change_idx < len(temp_data['value']) - 1:\n",
    "            optimal_temp = temp_data.iloc[max_change_idx]['value']\n",
    "            ax3_ent.axvline(x=optimal_temp, color='red', linestyle='--', alpha=0.7,\n",
    "                           label=f'Transition τ = {optimal_temp:.1f}')\n",
    "    \n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax3_ent.legend(lines, labels, loc='upper left')\n",
    "    ax3_ent.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Sinkhorn iterations analysis\n",
    "    ax4 = axes[1, 1]\n",
    "    sinkhorn_data = results_df[results_df['parameter'] == 'sinkhorn_iterations']\n",
    "    ax4_time = ax4  # Left y-axis for time\n",
    "    ax4_error = ax4.twinx()  # Right y-axis for error\n",
    "    \n",
    "    line1 = ax4_time.plot(sinkhorn_data['value'], sinkhorn_data['avg_time'] * 1000, \n",
    "                         'co-', linewidth=2, label='Computation Time', markersize=8)\n",
    "    ax4_time.set_xlabel('Sinkhorn Iterations')\n",
    "    ax4_time.set_ylabel('Time (ms)', color='cyan')\n",
    "    ax4_time.tick_params(axis='y', labelcolor='cyan')\n",
    "    ax4_time.set_xscale('log')  # Log scale for iterations\n",
    "    \n",
    "    line2 = ax4_error.plot(sinkhorn_data['value'], sinkhorn_data['avg_error'], \n",
    "                          'yo-', linewidth=2, label='Convergence Error', markersize=8)\n",
    "    ax4_error.set_ylabel('Convergence Error', color='orange')\n",
    "    ax4_error.tick_params(axis='y', labelcolor='orange')\n",
    "    \n",
    "    ax4_time.set_title('Sinkhorn Iterations: Time vs Accuracy Trade-off')\n",
    "    \n",
    "    # Find optimal iterations (knee point)\n",
    "    errors = np.array(sinkhorn_data['avg_error'])\n",
    "    times = np.array(sinkhorn_data['avg_time'] * 1000)\n",
    "    errors_norm = (errors - errors.min()) / (errors.max() - errors.min() + 1e-8)\n",
    "    times_norm = (times - times.min()) / (times.max() - times.min() + 1e-8)\n",
    "    distances = np.sqrt(errors_norm**2 + times_norm**2)  # Distance to ideal (0,0)\n",
    "    optimal_idx = np.argmin(distances)\n",
    "    optimal_iter = sinkhorn_data.iloc[optimal_idx]['value']\n",
    "    ax4_time.axvline(x=optimal_iter, color='red', linestyle='--', alpha=0.7,\n",
    "                    label=f'Optimal = {optimal_iter} iters')\n",
    "    \n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax4_time.legend(lines, labels, loc='upper right')\n",
    "    ax4_time.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Provide parameter recommendations\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PARAMETER TUNING RECOMMENDATIONS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"Identity Preservation Factor (λ):\")\n",
    "    print(f\"  Recommended: {optimal_value:.2f}\")\n",
    "    print(f\"  Reasoning: Balances individuality ({idf_data.iloc[optimal_idx]['avg_individuality']:.3f}) \"\n",
    "          f\"with stability ({idf_data.iloc[optimal_idx]['stability_rate']:.3f})\")\n",
    "    \n",
    "    # Signal bound recommendation\n",
    "    efficiency_threshold = 0.8\n",
    "    viable_bounds = bound_data[bound_data['avg_efficiency'] > efficiency_threshold]\n",
    "    if not viable_bounds.empty:\n",
    "        rec_bound = viable_bounds.iloc[0]['value']\n",
    "        print(f\"\\nSignal Bound (β):\")\n",
    "        print(f\"  Recommended: {rec_bound:.2f}\")\n",
    "        print(f\"  Reasoning: Provides {viable_bounds.iloc[0]['avg_efficiency']:.3f} efficiency \"\n",
    "              f\"with {viable_bounds.iloc[0]['avg_norm']:.3f} average norm\")\n",
    "    \n",
    "    # Temperature recommendation\n",
    "    print(f\"\\nTemperature (τ):\")\n",
    "    if 'optimal_temp' in locals():\n",
    "        print(f\"  Recommended: {optimal_temp:.1f}\")\n",
    "    else:\n",
    "        print(f\"  Recommended: 1.0\")\n",
    "    print(f\"  Reasoning: Balances attention uniformity with decision diversity\")\n",
    "    \n",
    "    # Sinkhorn iterations recommendation\n",
    "    print(f\"\\nSinkhorn Iterations:\")\n",
    "    print(f\"  Recommended: {optimal_iter}\")\n",
    "    print(f\"  Reasoning: Achieves {sinkhorn_data.iloc[optimal_idx]['avg_error']:.6f} error \"\n",
    "          f\"in {sinkhorn_data.iloc[optimal_idx]['avg_time']*1000:.3f} ms\")\n",
    "    \n",
    "    return fig, results_df\n",
    "\n",
    "# Cell 8: Adaptive mHC Variant\n",
    "class AdaptiveMHC(ManifoldConstrainedHyperConnections):\n",
    "    \"\"\"\n",
    "    Adaptive extension of mHC that learns optimal parameters during coordination.\n",
    "    \n",
    "    Key adaptive capabilities:\n",
    "    1. Learns identity preservation factor based on agent diversity\n",
    "    2. Adapts signal bound based on estimated threat severity\n",
    "    3. Adjusts temperature based on agent agreement levels\n",
    "    4. Maintains history for online learning and adaptation\n",
    "    \n",
    "    This enables mHC to automatically adjust to changing threat environments\n",
    "    and coordination requirements without manual parameter tuning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents: int, state_dim: int, learning_rate: float = 0.01):\n",
    "        \"\"\"\n",
    "        Initialize adaptive mHC with learnable parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        n_agents: Number of coordinating agents\n",
    "        state_dim: State vector dimensionality\n",
    "        learning_rate: Gradient descent learning rate for parameter adaptation\n",
    "        \"\"\"\n",
    "        super().__init__(n_agents, state_dim)\n",
    "        \n",
    "        # Define learnable parameters with PyTorch Parameter wrapper\n",
    "        self.identity_factor = nn.Parameter(torch.tensor(0.1))  # Learnable λ\n",
    "        self.signal_bound_param = nn.Parameter(torch.tensor(1.0))  # Learnable β\n",
    "        self.temperature_param = nn.Parameter(torch.tensor(1.0))  # Learnable τ\n",
    "        \n",
    "        # Adaptive learning components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = torch.optim.Adam([self.identity_factor, \n",
    "                                          self.signal_bound_param, \n",
    "                                          self.temperature_param], \n",
    "                                         lr=learning_rate)  # Adam optimizer for adaptation\n",
    "        \n",
    "        # History tracking for adaptation analysis\n",
    "        self.coordination_history = []\n",
    "        self.max_history = 1000  # Maximum history length for memory management\n",
    "    \n",
    "    def compute_adaptation_metrics(self, agent_states: List[torch.Tensor], \n",
    "                                 attention: torch.Tensor) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute environmental metrics for parameter adaptation.\n",
    "        \n",
    "        Analyzes current coordination context to determine:\n",
    "        1. Agent diversity (state variance)\n",
    "        2. Agent agreement (attention consensus)\n",
    "        3. Threat severity (state magnitudes)\n",
    "        \n",
    "        These metrics guide parameter adaptation decisions.\n",
    "        \"\"\"\n",
    "        # Stack states for batch processing\n",
    "        stacked_states = torch.stack(agent_states, dim=1)  # [B, N, D]\n",
    "        \n",
    "        # 1. Agent diversity: variance across agents\n",
    "        state_variance = stacked_states.var(dim=1).mean().item()  # Higher = more diverse\n",
    "        \n",
    "        # 2. Agent agreement: attention distribution uniformity\n",
    "        attention_flat = attention.flatten()\n",
    "        attention_entropy = -torch.sum(\n",
    "            attention_flat * torch.log(attention_flat + 1e-8)\n",
    "        ).item() / (self.n_agents * self.n_agents)  # Normalized entropy\n",
    "        \n",
    "        # 3. Threat severity: estimated from state magnitudes\n",
    "        state_magnitudes = torch.norm(stacked_states, dim=-1)  # [B, N]\n",
    "        avg_magnitude = state_magnitudes.mean().item()  # Average threat level\n",
    "        max_magnitude = state_magnitudes.max().item()  # Maximum threat level\n",
    "        \n",
    "        return {\n",
    "            'state_variance': state_variance,\n",
    "            'attention_entropy': attention_entropy,\n",
    "            'avg_magnitude': avg_magnitude,\n",
    "            'max_magnitude': max_magnitude\n",
    "        }\n",
    "    \n",
    "    def adapt_parameters(self, metrics: Dict, mixed_state: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Adapt parameters based on current coordination performance.\n",
    "        \n",
    "        Adaptation rules:\n",
    "        1. High diversity → higher identity preservation\n",
    "        2. High threat → tighter signal bound\n",
    "        3. Low agreement → higher temperature (smoother decisions)\n",
    "        \"\"\"\n",
    "        # Rule 1: Adapt identity factor based on diversity\n",
    "        # More diverse agents need more identity preservation\n",
    "        target_identity = min(0.3, max(0.05, metrics['state_variance'] * 2))\n",
    "        identity_loss = F.mse_loss(self.identity_factor, \n",
    "                                  torch.tensor(target_identity, device=self.identity_factor.device))\n",
    "        \n",
    "        # Rule 2: Adapt signal bound based on threat severity\n",
    "        # Higher threat requires more conservative (tighter) bounds\n",
    "        target_bound = min(2.0, max(0.5, 1.0 / (metrics['avg_magnitude'] + 0.5)))\n",
    "        bound_loss = F.mse_loss(self.signal_bound_param, \n",
    "                               torch.tensor(target_bound, device=self.signal_bound_param.device))\n",
    "        \n",
    "        # Rule 3: Adapt temperature based on agreement\n",
    "        # Low agreement (high entropy) needs higher temperature for smoother decisions\n",
    "        target_temp = min(5.0, max(0.5, metrics['attention_entropy'] * 10))\n",
    "        temp_loss = F.mse_loss(self.temperature_param, \n",
    "                              torch.tensor(target_temp, device=self.temperature_param.device))\n",
    "        \n",
    "        # Combine losses and update parameters\n",
    "        total_loss = identity_loss + bound_loss + temp_loss\n",
    "        self.optimizer.zero_grad()  # Clear previous gradients\n",
    "        total_loss.backward()  # Compute gradients\n",
    "        self.optimizer.step()  # Update parameters\n",
    "        \n",
    "        # Clamp parameters to valid ranges\n",
    "        self.identity_factor.data.clamp_(0.0, 0.5)\n",
    "        self.signal_bound_param.data.clamp_(0.1, 3.0)\n",
    "        self.temperature_param.data.clamp_(0.1, 10.0)\n",
    "        \n",
    "        # Update base class parameters\n",
    "        self.identity_preserve_factor = self.identity_factor.item()\n",
    "        self.signal_bound = self.signal_bound_param.item()\n",
    "        self.temperature = self.temperature_param.item()\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'identity_factor': self.identity_factor.item(),\n",
    "            'signal_bound': self.signal_bound_param.item(),\n",
    "            'temperature': self.temperature_param.item(),\n",
    "            'identity_loss': identity_loss.item(),\n",
    "            'bound_loss': bound_loss.item(),\n",
    "            'temp_loss': temp_loss.item()\n",
    "        }\n",
    "    \n",
    "    def convex_state_mixing(self, agent_states: List[torch.Tensor], \n",
    "                          attention_weights: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Override parent method to include parameter adaptation.\n",
    "        \n",
    "        Enhanced coordination flow:\n",
    "        1. Update parameters from learnable values\n",
    "        2. Compute adaptation metrics\n",
    "        3. Perform standard mHC mixing\n",
    "        4. Adapt parameters based on results\n",
    "        5. Store history for analysis\n",
    "        \"\"\"\n",
    "        # Sync parameters from learnable values\n",
    "        self.identity_preserve_factor = self.identity_factor.item()\n",
    "        self.signal_bound = self.signal_bound_param.item()\n",
    "        self.temperature = self.temperature_param.item()\n",
    "        \n",
    "        # Compute adaptation metrics from current context\n",
    "        metrics = self.compute_adaptation_metrics(agent_states, attention_weights)\n",
    "        \n",
    "        # Perform standard mHC mixing\n",
    "        mixed_state = super().convex_state_mixing(agent_states, attention_weights)\n",
    "        \n",
    "        # Adapt parameters based on coordination results\n",
    "        adaptation_results = self.adapt_parameters(metrics, mixed_state)\n",
    "        \n",
    "        # Store coordination history\n",
    "        self.coordination_history.append({\n",
    "            'metrics': metrics,\n",
    "            'adaptation': adaptation_results,\n",
    "            'mixed_state_norm': torch.norm(mixed_state).item()\n",
    "        })\n",
    "        \n",
    "        # Manage history length\n",
    "        if len(self.coordination_history) > self.max_history:\n",
    "            self.coordination_history = self.coordination_history[-self.max_history:]\n",
    "        \n",
    "        return mixed_state\n",
    "\n",
    "# Cell 9: GQA Integration\n",
    "class SimpleGQA(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Grouped Query Attention implementation for testing mHC integration.\n",
    "    \n",
    "    GQA reduces computation by sharing keys and values across groups of attention heads,\n",
    "    providing efficiency benefits while maintaining representational capacity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, n_groups: int = None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # Model dimension\n",
    "        self.n_heads = n_heads  # Number of attention heads\n",
    "        self.n_groups = n_groups if n_groups is not None else n_heads // 2\n",
    "        \n",
    "        # Validate configuration\n",
    "        assert n_heads % self.n_groups == 0, \"n_heads must be divisible by n_groups\"\n",
    "        \n",
    "        self.head_dim = d_model // n_heads  # Dimension per head\n",
    "        self.scale = self.head_dim ** -0.5  # Scaling factor for attention\n",
    "        \n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.q_proj = nn.Linear(d_model, d_model)  # Full dimension for queries\n",
    "        self.k_proj = nn.Linear(d_model, d_model // (n_heads // self.n_groups))  # Reduced for keys\n",
    "        self.v_proj = nn.Linear(d_model, d_model // (n_heads // self.n_groups))  # Reduced for values\n",
    "        self.out_proj = nn.Linear(d_model, d_model)  # Output projection\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Project inputs\n",
    "        Q = self.q_proj(query)  # [B, seq_len, d_model]\n",
    "        K = self.k_proj(key)    # [B, seq_len, d_model//group_ratio]\n",
    "        V = self.v_proj(value)  # [B, seq_len, d_model//group_ratio]\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.n_groups, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.n_groups, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Repeat K and V for each head in group (key sharing)\n",
    "        expand_ratio = self.n_heads // self.n_groups\n",
    "        K = K.repeat_interleave(expand_ratio, dim=1)\n",
    "        V = V.repeat_interleave(expand_ratio, dim=1)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn_weights = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape back to original format\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MHCGQAIntegration(nn.Module):\n",
    "    \"\"\"\n",
    "    Integrated architecture combining mHC with Grouped Query Attention.\n",
    "    \n",
    "    This provides a complete solution for efficient multi-agent coordination:\n",
    "    1. GQA for efficient intra-agent reasoning\n",
    "    2. mHC for stable inter-agent coordination\n",
    "    3. Adaptive parameter tuning for dynamic environments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents: int, d_model: int, n_heads: int, \n",
    "                 n_groups: int = None, use_mhc: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_agents = n_agents\n",
    "        self.d_model = d_model\n",
    "        self.use_mhc = use_mhc\n",
    "        \n",
    "        # GQA for efficient intra-agent attention\n",
    "        self.gqa_attention = SimpleGQA(d_model, n_heads, n_groups)\n",
    "        \n",
    "        # Adaptive mHC for inter-agent coordination\n",
    "        if use_mhc:\n",
    "            self.mhc_coordination = AdaptiveMHC(n_agents, d_model)\n",
    "        \n",
    "        # Agent feature encoder\n",
    "        self.agent_encoder = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),  # Expand dimension\n",
    "            nn.GELU(),  # GELU activation for smooth gradients\n",
    "            nn.Linear(d_model * 2, d_model),  # Compress back\n",
    "            nn.LayerNorm(d_model)  # Normalize\n",
    "        )\n",
    "        \n",
    "        # Coordination state decoder\n",
    "        self.coordination_decoder = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),  # Regularization\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        # Output heads for security tasks\n",
    "        self.threat_classifier = nn.Linear(d_model, 10)  # 10 threat types\n",
    "        self.severity_regressor = nn.Linear(d_model, 1)  # Continuous severity\n",
    "        self.confidence_estimator = nn.Linear(d_model, 1)  # Decision confidence\n",
    "    \n",
    "    def forward(self, agent_inputs: List[torch.Tensor], \n",
    "                agent_confidences: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Complete processing pipeline for security threat coordination.\n",
    "        \n",
    "        Steps:\n",
    "        1. Encode each agent's input features\n",
    "        2. Apply GQA self-attention for intra-agent reasoning\n",
    "        3. Coordinate across agents using mHC\n",
    "        4. Decode coordinated representation\n",
    "        5. Generate security assessments\n",
    "        \"\"\"\n",
    "        batch_size = agent_inputs[0].shape[0]\n",
    "        \n",
    "        # Step 1: Encode agent inputs\n",
    "        encoded_agents = []\n",
    "        for agent_input in agent_inputs:\n",
    "            encoded = self.agent_encoder(agent_input)  # [B, D]\n",
    "            encoded_agents.append(encoded)\n",
    "        \n",
    "        # Step 2: Intra-agent GQA attention\n",
    "        attended_agents = []\n",
    "        for encoded in encoded_agents:\n",
    "            # Self-attention for feature refinement\n",
    "            attended = self.gqa_attention(\n",
    "                encoded.unsqueeze(1),  # Add sequence dimension\n",
    "                encoded.unsqueeze(1),\n",
    "                encoded.unsqueeze(1)\n",
    "            ).squeeze(1)  # Remove sequence dimension\n",
    "            attended_agents.append(attended)\n",
    "        \n",
    "        # Step 3: Inter-agent mHC coordination\n",
    "        if self.use_mhc:\n",
    "            attention = torch.einsum('bi,bj->bij', agent_confidences, agent_confidences)\n",
    "            coordinated = self.mhc_coordination.convex_state_mixing(\n",
    "                attended_agents, attention\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: simple averaging\n",
    "            coordinated = torch.stack(attended_agents, dim=1).mean(dim=1)\n",
    "        \n",
    "        # Step 4: Decode coordinated state\n",
    "        decoded = self.coordination_decoder(coordinated)\n",
    "        \n",
    "        # Step 5: Generate security assessments\n",
    "        threat_logits = self.threat_classifier(decoded)  # Threat classification\n",
    "        severity = torch.sigmoid(self.severity_regressor(decoded))  # Severity score [0,1]\n",
    "        confidence = torch.sigmoid(self.confidence_estimator(decoded))  # Confidence score [0,1]\n",
    "        \n",
    "        # Extract mHC metrics for analysis\n",
    "        mhc_metrics = {}\n",
    "        if self.use_mhc and hasattr(self.mhc_coordination, 'coordination_history'):\n",
    "            if self.mhc_coordination.coordination_history:\n",
    "                latest = self.mhc_coordination.coordination_history[-1]\n",
    "                mhc_metrics = {\n",
    "                    'adaptation_loss': latest['adaptation']['total_loss'],\n",
    "                    'identity_factor': latest['adaptation']['identity_factor'],\n",
    "                    'signal_bound': latest['adaptation']['signal_bound'],\n",
    "                    'temperature': latest['adaptation']['temperature']\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            'threat_logits': threat_logits,\n",
    "            'severity': severity,\n",
    "            'confidence': confidence,\n",
    "            'coordinated_state': coordinated,\n",
    "            'agent_states': attended_agents,\n",
    "            'mhc_metrics': mhc_metrics\n",
    "        }\n",
    "    \n",
    "    def train_step(self, agent_inputs: List[torch.Tensor], \n",
    "                  agent_confidences: torch.Tensor,\n",
    "                  targets: Dict[str, torch.Tensor],\n",
    "                  optimizer: torch.optim.Optimizer) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Complete training step with multi-task loss calculation.\n",
    "        \n",
    "        Combines losses for:\n",
    "        1. Threat classification accuracy\n",
    "        2. Severity regression precision\n",
    "        3. Confidence calibration\n",
    "        4. mHC adaptation regularization\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        outputs = self(agent_inputs, agent_confidences)\n",
    "        \n",
    "        # Loss 1: Threat classification (cross-entropy)\n",
    "        threat_loss = F.cross_entropy(\n",
    "            outputs['threat_logits'], \n",
    "            targets['threat_labels'].long()\n",
    "        )\n",
    "        \n",
    "        # Loss 2: Severity regression (MSE)\n",
    "        severity_loss = F.mse_loss(\n",
    "            outputs['severity'], \n",
    "            targets['severity_labels']\n",
    "        )\n",
    "        \n",
    "        # Loss 3: Confidence calibration\n",
    "        predicted_classes = torch.argmax(outputs['threat_logits'], dim=-1)\n",
    "        correct = (predicted_classes == targets['threat_labels']).float()\n",
    "        confidence_loss = F.mse_loss(\n",
    "            outputs['confidence'].squeeze(),\n",
    "            correct  # Confidence should match accuracy\n",
    "        )\n",
    "        \n",
    "        # Loss 4: mHC adaptation regularization\n",
    "        mhc_reg = 0.0\n",
    "        if self.use_mhc and outputs['mhc_metrics']:\n",
    "            # Regularize parameters to prevent extreme values\n",
    "            identity_reg = torch.abs(outputs['mhc_metrics']['identity_factor'] - 0.2)\n",
    "            bound_reg = torch.abs(outputs['mhc_metrics']['signal_bound'] - 1.0)\n",
    "            temp_reg = torch.abs(outputs['mhc_metrics']['temperature'] - 1.0)\n",
    "            mhc_reg = (identity_reg + bound_reg + temp_reg) / 3\n",
    "        \n",
    "        # Total loss with weighting\n",
    "        total_loss = (threat_loss + severity_loss + \n",
    "                     confidence_loss * 0.5 + mhc_reg * 0.1)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Return comprehensive training metrics\n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'threat_loss': threat_loss.item(),\n",
    "            'severity_loss': severity_loss.item(),\n",
    "            'confidence_loss': confidence_loss.item(),\n",
    "            'mhc_reg': mhc_reg.item() if isinstance(mhc_reg, torch.Tensor) else mhc_reg,\n",
    "            'threat_accuracy': (predicted_classes == targets['threat_labels']).float().mean().item(),\n",
    "            'severity_mae': torch.abs(outputs['severity'] - targets['severity_labels']).mean().item(),\n",
    "            'confidence_calibration': torch.abs(outputs['confidence'].squeeze() - correct).mean().item()\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
